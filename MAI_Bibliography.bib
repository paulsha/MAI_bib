@misc{,
title = {{Amazon Web Services (AWS) - Cloud Computing Services}},
url = {https://aws.amazon.com/},
urldate = {2018-05-01}
}
@misc{,
title = {{Google Cloud Computing, Hosting Services {\&} APIs | Google Cloud}},
url = {https://cloud.google.com/},
urldate = {2018-05-01}
}
@misc{,
title = {{Gluon API https://github.com/gluon-api/gluon-api}},
url = {https://github.com/gluon-api/gluon-api},
urldate = {2018-04-09}
}
@inproceedings{He2016,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learn- ing residual functions with reference to the layer inputs, in- stead of learning unreferenced functions. We provide com- prehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complex- ity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our ex- tremely deep representations, we obtain a 28{\%} relative im- provement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet local- ization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.90},
eprint = {1512.03385},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - Unknown - Deep Residual Learning for Image Recognition.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {1664-1078},
pages = {770--778},
pmid = {23554596},
title = {{Deep Residual Learning for Image Recognition}},
url = {https://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2016/papers/He{\_}Deep{\_}Residual{\_}Learning{\_}CVPR{\_}2016{\_}paper.pdf http://ieeexplore.ieee.org/document/7780459/},
year = {2016}
}
@misc{Pitie,
author = {Pitie, Francois},
title = {{Machine Learning with Applications in Media Engineering (TCD, EE5M16)}},
url = {https://github.com/frcs/EE4C16},
urldate = {2018-05-01}
}
@article{Kingma2017,
abstract = {The identification of seizure activities in non-stationary electroencephalography (EEG) is a challenging task. The seizure detection by human inspection of EEG signals is prone to errors, inaccurate as well as time-consuming. Several attempts have been made to develop automatic systems so as to assist neurophysiologists in identifying epileptic seizures accurately. The proposed study brings forth a novel automatic approach to detect epileptic seizures using analytic time-frequency flexible wavelet transform (ATFFWT) and fractal dimension (FD). The ATFFWT has inherent attractive features such as, shift-invariance property, tunable oscillatory attribute and flexible time-frequency covering favorable for the analysis of non-stationary and transient signals. We have used ATFFWT to decompose EEG signals into the desired subbands. Following the ATFFWT decomposition, we calculate FD for each subband. Finally, FDs of all subbands have been fed to the least-squares support vector machine (LS-SVM) classifier. The 10-fold cross validation has been used to obtain stable and reliable performance and to avoid the over fitting of the model. In this study, we investigate various different classification problems (CPs) pertaining to different classes of EEG signals, including the following popular CPs: (i) ictal versus normal (ii) ictal versus inter-ictal (iii) ictal versus non-ictal. The proposed model is found to be outperforming all existing models in terms of classification sensitivity (CSE) as it achieves perfect 100{\%} sensitivity for seven CPs investigated by us. The prominent attribute of the proposed system is that though the model employs only one set of discriminating features (FD) for all CPs, it yields promising classification accuracy. Since, the proposed model attains the perfect classification performance it appears that a system is in place to assist clinicians to diagnose seizures accurately in less time. Further, the proposed system seems useful and attractive, especially, in the rural areas of developing countries where there is a shortage of experienced clinicians and expensive machines like functional magnetic resonance imaging (fMRI).},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.6980v9},
author = {Kingma, Diederik P},
doi = {10.1016/j.patrec.2017.03.023},
eprint = {arXiv:1412.6980v9},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kingma, Ba - Unknown - ADAM A METHOD FOR STOCHASTIC OPTIMIZATION.pdf:pdf},
isbn = {0925-2312 1872-8286},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Analytic time-frequency flexible wavelet transform,EEG,Electroencephalography,Epilepsy,Fractal dimension,Seizure},
pages = {172--179},
pmid = {16023730},
title = {{ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION}},
url = {https://arxiv.org/pdf/1412.6980.pdf},
volume = {94},
year = {2017}
}
@misc{,
title = {{CS231n Convolutional Neural Networks for Visual Recognition}},
url = {http://cs231n.github.io/},
urldate = {2018-05-08}
}
@incollection{Li2012,
author = {Li, Jing and Cheng, Ji-hang and Shi, Jing-yuan and Huang, Fei},
doi = {10.1007/978-3-642-30223-7_87},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2012 - Brief Introduction of Back Propagation (BP) Neural Network Algorithm and Its Improvement.pdf:pdf},
pages = {553--558},
publisher = {Springer, Berlin, Heidelberg},
title = {{Brief Introduction of Back Propagation (BP) Neural Network Algorithm and Its Improvement}},
url = {http://link.springer.com/10.1007/978-3-642-30223-7{\_}87},
year = {2012}
}
@misc{,
title = {{Machine Learning Glossary  |  Google Developers}},
url = {https://developers.google.com/machine-learning/glossary/},
urldate = {2018-05-08}
}
@misc{,
title = {{Adages and Coinages | Larry Tesler}},
url = {http://www.nomodes.com/Larry{\_}Tesler{\_}Consulting/Adages{\_}and{\_}Coinages.html},
urldate = {2018-05-08}
}
@article{Redmon2015,
abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
archivePrefix = {arXiv},
arxivId = {1506.02640},
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
doi = {10.1109/CVPR.2016.91},
eprint = {1506.02640},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Redmon et al. - 2015 - You Only Look Once Unified, Real-Time Object Detection(2).pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {01689002},
pmid = {27295650},
title = {{You Only Look Once: Unified, Real-Time Object Detection}},
url = {http://pjreddie.com/yolo/ http://arxiv.org/abs/1506.02640},
year = {2015}
}
@article{Huang,
abstract = {Evaluation measures play an important role in ma-chine learning because they are used not only to compare different learning algorithms, but also of-ten as goals to optimize in constructing learning models. Both formal and empirical work has been published in comparing evaluation measures. In this paper, we propose a general approach to con-struct new measures based on the existing ones, and we prove that the new measures are consis-tent with, and finer than, the existing ones. We also show that the new measure is more correlated to RMS (Root Mean Square error) with artificial datasets. Finally, we demonstrate experimentally that the greedy-search based algorithm (such as ar-tificial neural networks) trained with the new and finer measure usually can achieve better prediction performance. This provides a general approach to improve the predictive performance of existing learning algorithms based on greedy search.},
author = {Huang, Jin and Ling, Charles X},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang, Ling - Unknown - Constructing New and Better Evaluation Measures for Machine Learning.pdf:pdf},
keywords = {heuristics,learning},
title = {{Constructing New and Better Evaluation Measures for Machine Learning}},
url = {https://pdfs.semanticscholar.org/e399/8d8083baf502cd6218c606c03cb34ad036c8.pdf}
}
@article{Chawla2004,
abstract = {methods for bal- ancing machine training . digital text categorization from docu- ments Toward scalable with non-uniform class},
archivePrefix = {arXiv},
arxivId = {10.1007/978-0-387-09823-4{\_}45},
author = {Chawla, Nitesh V and Japkowicz, Nathalie and Drive, Prentice},
doi = {http://doi.acm.org/10.1145/1007730.1007733},
eprint = {978-0-387-09823-4{\_}45},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chawla, Japkowicz, Ko - Unknown - Editorial Special Issue on Learning from Imbalanced Data Sets.pdf:pdf},
isbn = {978-0-387-24435-8},
issn = {1931-0145},
journal = {ACM SIGKDD Explorations Newsletter},
number = {1},
pages = {1--6},
pmid = {25246403},
primaryClass = {10.1007},
title = {{Editorial : Special Issue on Learning from Imbalanced Data Sets}},
url = {https://www3.nd.edu/{~}dial/publications/chawla2004editorial.pdf},
volume = {6},
year = {2004}
}
@inproceedings{Ranawana,
abstract = {All learning algorithms attempt to improve the accuracy of a classification system. However, the effectiveness of such a system is dependent on the heuristic used by the learning paradigm to measure performance. This paper demonstrates that the use of Precision (P) for performance evaluation of imbalanced data sets could lead the solution towards sub-optimal answers. We move onto present a novel performance heuristic, the 'Optimized Precision (OP)', to negate these detrimental effects. We also analyze the impact of these observations on the training performance of ensemble learners and Multi-Classifier Systems (MCS), and provide guidelines for the proper training of multi-classifier systems.},
author = {Ranawana, R. and Palade, V.},
booktitle = {2006 IEEE International Conference on Evolutionary Computation},
doi = {10.1109/CEC.2006.1688586},
isbn = {0-7803-9487-9},
issn = {1089-778X},
pages = {2254--2261},
publisher = {IEEE},
title = {{Optimized Precision - A New Measure for Classifier Performance Evaluation}},
url = {http://ieeexplore.ieee.org/document/1688586/}
}
@misc{,
title = {{Royal Victoria Hospital}},
url = {http://www.belfasttrust.hscni.net/hospitals/RVHIntro.htm},
urldate = {2018-04-04}
}
@misc{,
title = {{:: Anaconda Cloud}},
url = {https://anaconda.org/},
urldate = {2018-04-05}
}
@article{Pareto1897,
annote = {doi: 10.1177/000271629700900314},
author = {Pareto, Vifredo},
doi = {10.1177/000271629700900314},
issn = {0002-7162},
journal = {The ANNALS of the American Academy of Political and Social Science},
month = {nov},
number = {3},
pages = {128--131},
publisher = {SAGE Publications Inc},
title = {{Cours d'{\'{E}}conomie Politique. By VILFREDO PARETO, Professeur {\`{a}} l'Universit{\'{e}} de Lausanne. Vol. I. Pp. 430. I896. Vol. II. Pp. 426. I897. Lausanne: F. Rouge}},
url = {https://doi.org/10.1177/000271629700900314},
volume = {9},
year = {1897}
}
@misc{Ng2011,
abstract = {Why write programs when the computer can instead learn them from data? In this class you will learn how to make this happen, from the simplest machine learning algorithms to quite sophisticated ones. Enjoy!},
author = {Ng, Andrew},
title = {{Machine Learning Coursera}},
url = {https://www.coursera.org/learn/machine-learning https://www.coursera.org/learn/machine-learning/home/welcome},
year = {2011}
}
@misc{,
title = {{LabelImg is a graphical image annotation tool and label object bounding boxes in images}},
url = {https://github.com/tzutalin/labelImg},
urldate = {2018-04-04}
}
@inproceedings{JiaDeng2009,
abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called {\&}{\#}x201C;ImageNet{\&}{\#}x201D;, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
author = {{Jia Deng} and {Wei Dong} and Socher, Richard and {Li-Jia Li} and {Kai Li} and {Li Fei-Fei}},
booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPRW.2009.5206848},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Deng et al. - Unknown - ImageNet A Large-Scale Hierarchical Image Database.pdf:pdf},
isbn = {978-1-4244-3992-8},
issn = {1063-6919},
pages = {248--255},
pmid = {21914436},
title = {{ImageNet: A large-scale hierarchical image database}},
url = {http://www.image-net.org/papers/imagenet{\_}cvpr09.pdf http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5206848},
year = {2009}
}
@inproceedings{Kagaya2014,
address = {New York, New York, USA},
author = {Kagaya, Hokuto and Aizawa, Kiyoharu and Ogawa, Makoto},
booktitle = {Proceedings of the ACM International Conference on Multimedia - MM '14},
doi = {10.1145/2647868.2654970},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kagaya, Aizawa, Ogawa - 2014 - Food Detection and Recognition Using Convolutional Neural Network.pdf:pdf},
isbn = {9781450330633},
keywords = {convolutional neural network,deep learning,food detection,food recognition},
pages = {1085--1088},
publisher = {ACM Press},
title = {{Food Detection and Recognition Using Convolutional Neural Network}},
url = {http://dl.acm.org/citation.cfm?doid=2647868.2654970},
year = {2014}
}
@misc{,
title = {{Python Software Foundation | Python Software Foundation}},
url = {https://www.python.org/psf/},
urldate = {2018-04-03}
}
@misc{,
title = {{Custom Search JSON/Atom API | Custom Search | Google Developers}},
url = {https://developers.google.com/custom-search/json-api/v1/overview},
urldate = {2018-04-04}
}
@misc{,
title = {{Flickr}},
url = {https://www.flickr.com/},
urldate = {2018-04-04}
}
@misc{,
title = {{Instagram}},
url = {https://www.instagram.com/},
urldate = {2018-04-04}
}
@misc{,
title = {{Google Images}},
url = {https://images.google.com/},
urldate = {2018-04-04}
}
@article{Lin2017,
abstract = {0 0.2 0.4 0.6 0.8 1 probability of ground truth class 0 1 2 3 4 5 loss = 0 = 0.5 = 1 = 2 = 5 well-classiied examples well-classiied examples CE(pt) = − log(pt) FL(pt) = −(1 − pt) $\gamma$ log(pt) Figure 1. We propose a novel loss we term the Focal Loss that adds a factor (1 − pt) $\gamma$ to the standard cross entropy criterion. Setting $\gamma$ {\textgreater} 0 reduces the relative loss for well-classified examples (pt {\textgreater} .5), putting more focus on hard, misclassified examples. As our experiments will demonstrate, the proposed focal loss enables training highly accurate dense object detectors in the presence of vast numbers of easy background examples. Abstract The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object lo-cations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the ex-treme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelm-ing the detector during training. To evaluate the effective-ness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of pre-vious one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at:},
archivePrefix = {arXiv},
arxivId = {arXiv:1708.02002v2},
author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'{a}}r, Piotr},
doi = {10.1109/ICCV.2017.324},
eprint = {arXiv:1708.02002v2},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Amarenco et al. - 2004 - Statins in stroke prevention and carotid atherosclerosis Systematic review and up-to-date meta-analysis.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {arXiv preprint arXiv:1708.02002},
pmid = {23766329},
title = {{Focal loss for dense object detection}},
url = {https://github.com/facebookresearch/Detectron.},
year = {2017}
}
@article{Redmon2018,
abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
archivePrefix = {arXiv},
arxivId = {1804.02767},
author = {Redmon, Joseph and Farhadi, Ali},
doi = {10.1109/CVPR.2017.690},
eprint = {1804.02767},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Redmon, Farhadi - 2018 - YOLOv3 An Incremental Improvement.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {0146-4833},
pmid = {23021419},
title = {{YOLOv3: An Incremental Improvement}},
url = {https://pjreddie.com/media/files/papers/YOLOv3.pdf http://arxiv.org/abs/1804.02767},
year = {2018}
}
@article{Redmon2015a,
abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
archivePrefix = {arXiv},
arxivId = {1506.02640},
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
eprint = {1506.02640},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Redmon et al. - 2015 - You Only Look Once Unified, Real-Time Object Detection(2).pdf:pdf},
month = {jun},
title = {{You Only Look Once: Unified, Real-Time Object Detection}},
url = {http://arxiv.org/abs/1506.02640},
year = {2015}
}
@article{Shrivastava2016,
abstract = {The field of object detection has made significant advances riding on the wave of region-based ConvNets, but their training procedure still includes many heuristics and hyperparameters that are costly to tune. We present a simple yet surprisingly effective online hard example mining (OHEM) algorithm for training region-based ConvNet detectors. Our motivation is the same as it has always been -- detection datasets contain an overwhelming number of easy examples and a small number of hard examples. Automatic selection of these hard examples can make training more effective and efficient. OHEM is a simple and intuitive algorithm that eliminates several heuristics and hyperparameters in common use. But more importantly, it yields consistent and significant boosts in detection performance on benchmarks like PASCAL VOC 2007 and 2012. Its effectiveness increases as datasets become larger and more difficult, as demonstrated by the results on the MS COCO dataset. Moreover, combined with complementary advances in the field, OHEM leads to state-of-the-art results of 78.9{\%} and 76.3{\%} mAP on PASCAL VOC 2007 and 2012 respectively.},
archivePrefix = {arXiv},
arxivId = {1604.03540},
author = {Shrivastava, Abhinav and Gupta, Abhinav and Girshick, Ross},
doi = {10.1109/CVPR.2016.89},
eprint = {1604.03540},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shrivastava, Gupta, Girshick - 2016 - Training Region-based Object Detectors with Online Hard Example Mining.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {10636919},
title = {{Training Region-based Object Detectors with Online Hard Example Mining}},
url = {https://arxiv.org/pdf/1604.03540.pdf http://arxiv.org/abs/1604.03540},
year = {2016}
}
@incollection{Rothe2015,
abstract = {Non-maximum suppression (NMS) is a key post-processing step in many computer vision applications. In the context of object detection, it is used to transform a smooth response map that triggers many imprecise object window hypotheses in, ideally, a single bounding-box for each detected object. The most common approach for NMS for object detection is a greedy, locally optimal strategy with several hand-designed components (e.g., thresholds). Such a strategy inherently suffers from several shortcomings, such as the inability to detect nearby objects. In this paper, we try to alleviate these problems and explore a novel formulation of NMS as a well-defined clustering problem. Our method builds on the recent Affinity Propagation Clustering algorithm, which passes messages between data points to identify cluster exemplars. Contrary to the greedy approach, our method is solved globally and its parameters can be automatically learned from training data. In experiments, we show in two contexts -- object class and generic object detection -- that it provides a promising solution to the shortcomings of the greedy NMS.},
author = {Rothe, Rasmus and Guillaumin, Matthieu and {Van Gool}, Luc},
booktitle = {Asian Conference on Computer Vision (ACCV)},
doi = {10.1007/978-3-319-16865-4_19},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rothe, Guillaumin, Van Gool - 2015 - Non-maximum Suppression for Object Detection by Passing Messages Between Windows.pdf:pdf},
isbn = {978-3-319-16865-4},
keywords = {NMS},
pages = {290--306},
title = {{Non-maximum Suppression for Object Detection by Passing Messages Between Windows}},
url = {https://www.vision.ee.ethz.ch/publications/papers/proceedings/eth{\_}biwi{\_}01126.pdf https://doi.org/10.1007/978-3-319-16865-4{\_}19},
year = {2015}
}
@inproceedings{Liu2015,
abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For {\$}300\backslashtimes 300{\$} input, SSD achieves 72.1{\%} mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for {\$}500\backslashtimes 500{\$} input, SSD achieves 75.1{\%} mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
archivePrefix = {arXiv},
arxivId = {1512.02325},
author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng Yang and Berg, Alexander C.},
doi = {10.1007/978-3-319-46448-0_2},
eprint = {1512.02325},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2015 - SSD Single shot multibox detector.pdf:pdf},
isbn = {9783319464473},
issn = {16113349},
keywords = {Convolutional neural network,Real-time object detection},
month = {dec},
pages = {21--37},
pmid = {23739795},
title = {{SSD: Single shot multibox detector}},
url = {http://arxiv.org/abs/1512.02325 http://dx.doi.org/10.1007/978-3-319-46448-0{\_}2},
volume = {9905 LNCS},
year = {2015}
}
@inproceedings{Girshick2015,
abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
archivePrefix = {arXiv},
arxivId = {1504.08083},
author = {Girshick, Ross},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2015.169},
eprint = {1504.08083},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Girshick - 2015 - Fast R-CNN.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
month = {apr},
pages = {1440--1448},
pmid = {23739795},
title = {{Fast R-CNN}},
url = {http://arxiv.org/abs/1504.08083},
volume = {2015 Inter},
year = {2015}
}
@inproceedings{Girshick2013,
abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30{\%} relative to the previous best result on VOC 2012---achieving a mAP of 53.3{\%}. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/{\~{}}rbg/rcnn.},
archivePrefix = {arXiv},
arxivId = {1311.2524},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.81},
eprint = {1311.2524},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Girshick et al. - 2013 - Rich feature hierarchies for accurate object detection and semantic segmentation.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
pages = {580--587},
pmid = {26656583},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
url = {http://www.cs.berkeley.edu},
year = {2013}
}
@article{Uijlings2012,
abstract = {act er addresses the problem of generating possible object lo-or use in object recognition. We introduce Selective Search mbines the strength of both an exhaustive search and seg-n. Like segmentation, we use the image structure to guide pling process. Like exhaustive search, we aim to capture ble object locations. Instead of a single technique to gen-ssible object locations, we diversify our search and use a f complementary image partitionings to deal with as many onditions as possible. Our Selective Search results in a t of data-driven, class-independent, high quality locations, (a) (b)},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Uijlings, J R R and {Van De Sande}, K E A and Gevers, T and Smeulders, A W M},
doi = {10.1007/s11263-013-0620-5},
eprint = {1409.4842},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Uijlings et al. - 2012 - Selective Search for Object Recognition.pdf:pdf},
isbn = {9781457711015},
issn = {09205691},
keywords = {()},
pmid = {24920543},
title = {{Selective Search for Object Recognition}},
url = {https://koen.me/research/pub/uijlings-ijcv2013-draft.pdf http://www.cs.cornell.edu/courses/cs7670/2014sp/slides/VisionSeminar14.pdf},
year = {2012}
}
@article{Ren2017,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region pro-posal computation as a bottleneck. In this work, we introduce a Region Pro-posal Network (RPN) that shares full-image convolutional features with the de-tection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and ob-jectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model [19], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2{\%} mAP) and 2012 (70.4{\%} mAP) using 300 proposals per image. Code is available at https://github.com/ShaoqingRen/faster{\_}rcnn.},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ren et al. - 2017 - Faster R-CNN Towards Real-Time Object Detection with.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {6},
pages = {1137--1149},
title = {{Faster R-CNN: Towards Real-Time Object Detection with}},
url = {https://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf},
volume = {39},
year = {2017}
}
@article{Rush2017,
abstract = {Deep learning and complex machine learning has quickly become one of the most important computationally intensive applications for a wide variety of fields. The combination of large data sets, high-performance computational capabilities, and evolving and improving algorithms has enabled many successful applications which were previously difficult or impossible to consider. This paper explores the challenges of deep learning training and inference, and discusses the benefits of a comprehensive approach for combining CPU, GPU, FPGA technologies, along with the appropriate software frameworks in a unified deep learning architecture. Each of these hardware technologies offers unique benefits to the deep learning problem, and a properly designed system can take advantage of this combination. Moreover, the combination can provide unique capabilities that result in higher performance, better efficiency, greater flexibility, and a hedge against algorithm obsolescence compared to CPU/GPU and FPGA systems designed separately. Aside from the underlying hardware approaches, a unified software environment is necessary to provide a clean interface to the application layer. This needs to account for several factors, including framework support, different compiler and code generator technologies, and optimization support for the underlying hardware engines. Higher-level frameworks (e.g., TensorFlow, Theano) can effectively hide most heterogeneity from application developers as well as enable portability across different systems. This is a powerful enabler for heterogeneous hardware. For application developers working below the framework level, the AMD ROCm and MIopen software frameworks are discussed as an example of a unified software environment applicable to a CPU and GPU solution. FPGAs are primarily used for inference, and the xfDNN middleware from Xilinx captures the software features essential for implementing deep learning inference on FPGAs. A long-term vision for application developers is a full and seamless programing environment that works across CPUs, GPUs, and FPGAs. This could initially focus on support for a common language and runtime, such as OpenCL, and later be extended to additional languages. The language support would hide any internal differences in compilers and runtimes between the CPU, GPU, and FPGA implementations. This seamless programming environment will facilitate the full end-to-end optimization of resource allocation. 2 Background},
author = {Rush, Allen and Sirasao, Ashish and Ignatowski, Mike},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rush, Sirasao, Ignatowski - 2017 - Unified Deep Learning with CPU, GPU, and FPGA Technologies.pdf:pdf},
pages = {1--10},
title = {{Unified Deep Learning with CPU, GPU, and FPGA Technologies}},
url = {https://pro.radeon.com/{\_}downloads/Unified-Deep-Learning-White-Paper.pdf},
year = {2017}
}
@misc{,
title = {{SLURM | UIT High-Performance Computing}},
url = {https://hpc.unt.edu/slurm-software},
urldate = {2018-05-03}
}
@inproceedings{Jette2003,
abstract = {Simple Linux Utility for Resource Management (SLURM) is an open source, fault-tolerant, and highly scalable cluster management and job scheduling system for Linux clusters of thousands of nodes. Components include machine status, partition management, job management, scheduling and stream copy modules. The design also includes a scalable, general-purpose communication infrastructure. This paper presents a overview of the SLURM architecture and functionality.},
author = {Jette, Morris and Grondona, Mark},
booktitle = {ClusterWorld Conference and Expo CWCE},
doi = {10.1007/10968987},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jette, Grondona - 2003 - SLURM Simple Linux Utility for Resource Management.pdf:pdf},
isbn = {978-3-540-20405-3},
issn = {03029743},
keywords = {scheduling},
pages = {44--60},
title = {{SLURM: Simple Linux Utility for Resource Management}},
url = {https://slurm.schedmd.com/slurm{\_}design.pdf http://www.springerlink.com/content/c4pgx63utdajtuwn/},
volume = {2682},
year = {2003}
}
@misc{Chollet2015,
abstract = {Keras: The Python Deep Learning library You have just found Keras. Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research. Use Keras if you need a deep learning library that: Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility). Supports both convolutional networks and recurrent networks, as well as combinations of the two. Runs seamlessly on CPU and GPU. Read the documentation at Keras.io. Keras is compatible with: Python 2.7-3.6.},
author = {Chollet, Fran{\c{c}}ois},
booktitle = {Keras.Io},
title = {{Keras Documentation}},
url = {https://keras.io/ https://keras.io/{\%}0Ahttps://keras.io},
urldate = {2018-05-03},
year = {2015}
}
@techreport{Yu2015,
abstract = {We introduce computational network (CN), a unified framework for describing arbitrary learning machines, such as deep neural networks (DNNs), convolutional neural networks (CNNs), recurrent neural networks (RNNs), long short term memory (LSTM), logistic regression, and maximum entropy model, that can be illustrated as a series of computational steps. A CN is a directed graph in which each leaf node represents an input value or a parameter and each non-leaf node represents a matrix operation upon its children. We describe algorithms to carry out forward computation and gradient calculation in CN and introduce most popular computation node types used in a typical CN. We further introduce the computational network toolkit (CNTK), an implementation of CN that supports both GPU and CPU. We describe the architecture and the key components of the CNTK, the command line options to use CNTK, and the network definition and model editing language, and provide sample setups for acoustic model, language model, and spoken language understanding. We also describe the Argon speech recognition decoder as an example to integrate with CNTK.},
author = {Yu, Dong and Eversole, Adam and Seltzer, Mike and Yao, Kaisheng and Huang, Zhiheng and Guenter, Brian and Kuchaiev, Oleksii and Zhang, Yu and Seide, Frank and Wang, Huaming and Droppo, Jasha and Zweig, Geoffrey and Rossbach, Chris and Currey, Jon and Gao, Jie and May, Avner and Peng, Baolin and Stolcke, Andreas and Slaney, Malcolm},
booktitle = {Microsoft Technical Report},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yu et al. - 2015 - An Introduction to Computational Networks and the Computational Network Toolkit.pdf:pdf},
number = {MSR-TR-2014-112},
title = {{An Introduction to Computational Networks and the Computational Network Toolkit}},
url = {https://www.microsoft.com/en-us/research/wp-content/uploads/2014/08/CNTKBook-20160217.pdf},
year = {2015}
}
@misc{,
title = {{TensorBoard: Visualizing Learning  |  TensorFlow}},
url = {https://www.tensorflow.org/programmers{\_}guide/summaries{\_}and{\_}tensorboard},
urldate = {2018-05-03}
}
@misc{,
title = {{MILA and the future of Theano - Google Groups}},
url = {https://groups.google.com/forum/{\#}!msg/theano-users/7Poq8BZutbY/rNCIfvAEAwAJ},
urldate = {2018-05-03}
}
@misc{,
title = {{Stack Overflow Developer Survey 2018}},
url = {https://insights.stackoverflow.com/survey/2018},
urldate = {2018-05-03}
}
@article{Abadi2016,
abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with particularly strong support for training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model in contrast to existing systems, and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.},
archivePrefix = {arXiv},
arxivId = {1605.08695},
author = {Abadi, Mart{\'{i}}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
doi = {10.1038/nn.3331},
eprint = {1605.08695},
isbn = {978-1-931971-33-1},
issn = {0270-6474},
pmid = {16411492},
title = {{TensorFlow: A system for large-scale machine learning}},
url = {https://research.google.com/pubs/pub45381.html http://arxiv.org/abs/1605.08695},
year = {2016}
}
@techreport{Collobert2011,
abstract = {Torch7 is a versatile numeric computing framework and machine learning library that extends Lua. Its goal is to provide a flexible environment to design and train learning machines. Flexibility is obtained via Lua, an extremely lightweight scripting language. High performance is obtained via efficient OpenMP/SSE and CUDA implementations of low-level numeric routines. Torch7 can easily be in- terfaced to third-party software thanks to Lua's light interface.},
author = {Collobert, Ronan and Kavukcuoglu, Koray and Farabet, Cl{\'{e}}ment},
booktitle = {BigLearn, NIPS Workshop},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Collobert, Kavukcuoglu, Farabet - 2011 - Torch7 A matlab-like environment for machine learning.pdf:pdf},
pages = {1--6},
title = {{Torch7: A matlab-like environment for machine learning}},
url = {https://infoscience.epfl.ch/record/192376/files/Collobert{\_}NIPSWORKSHOP{\_}2011.pdf; http://infoscience.epfl.ch/record/192376/files/Collobert{\_}NIPSWORKSHOP{\_}2011.pdf},
year = {2011}
}
@inproceedings{Pittaras2017,
abstract = {Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU ({\$}\backslashapprox{\$} 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.},
archivePrefix = {arXiv},
arxivId = {1408.5093},
author = {Pittaras, Nikiforos and Markatopoulou, Foteini and Mezaris, Vasileios and Patras, Ioannis},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-51811-4_9},
eprint = {1408.5093},
isbn = {9783319518107},
issn = {16113349},
keywords = {Concept detection,Deep learning,Visual analysis},
month = {jun},
pages = {102--114},
title = {{Comparison of fine-tuning and extension strategies for deep convolutional neural networks}},
url = {http://arxiv.org/abs/1408.5093},
volume = {10132 LNCS},
year = {2017}
}
@article{TheanoDevelopmentTeam2016,
abstract = {Theano is a Python library that allows to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Since its introduction, it has been one of the most used CPU and GPU mathematical compilers - especially in the machine learning community - and has shown steady performance improvements. Theano is being actively and continuously developed since 2008, multiple frameworks have been built on top of it and it has been used to produce many state-of-the-art machine learning models. The present article is structured as follows. Section I provides an overview of the Theano software and its community. Section II presents the principal features of Theano and how to use them, and compares them with other similar projects. Section III focuses on recently-introduced functionalities and improvements. Section IV compares the performance of Theano against Torch7 and TensorFlow on several machine learning models. Section V discusses current limitations of Theano and potential ways of improving it.},
archivePrefix = {arXiv},
arxivId = {1605.02688},
author = {{Theano Development Team}},
doi = {1605.02688},
eprint = {1605.02688},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Theano Development Team - 2016 - Theano A Python framework for fast computation of mathematical expressions.pdf:pdf},
journal = {arXiv e-prints},
month = {may},
pages = {19},
title = {{Theano: A Python framework for fast computation of mathematical expressions}},
url = {http://arxiv.org/abs/1605.02688},
year = {2016}
}
@article{Heung2016,
abstract = {Machine-learning is the automated process of uncovering patterns in large datasets using computer-based statistical models, where a fitted model may then be used for prediction purposes on new data. Despite the growing number of machine-learning algorithms that have been developed, relatively few studies have provided a comparison of an array of different learners - typically, model comparison studies have been restricted to a comparison of only a few models. This study evaluates and compares a suite of 10 machine-learners as classification algorithms for the prediction of soil taxonomic units in the Lower Fraser Valley, British Columbia, Canada. A variety of machine-learners (CART, CART with bagging, Random Forest, k-nearest neighbor, nearest shrunken centroid, artificial neural network, multinomial logistic regression, logistic model trees, and support vector machine) were tested in the extraction of the complex relationships between soil taxonomic units (great groups and orders) from a conventional soil survey and a suite of 20 environmental covariates representing the topography, climate, and vegetation of the study area. Methods used to extract training data from a soil survey included by-polygon, equal-class, area-weighted, and area-weighted with random over sampling (ROS) approaches. The fitted models, which consist of the soil-environmental relationships, were then used to predict soil great groups and orders for the entire study area at a 100 m spatial resolution. The resulting maps were validated using 262 points from legacy soil data. On average, the area-weighted sampling approach for developing training data from a soil survey was most effective. Using a validation of R= 1 cell, the k-nearest neighbor and support vector machine with radial basis function resulted in the highest accuracy of 72{\%} for great groups using ROS; however, models such as CART with bagging, logistic model trees, and Random Forest were preferred due to the speed of parameterization and the interpretability of the results while resulting in similar accuracies ranging from 65-70{\%} using the area-weighted sampling approach. Model choice and sample design greatly influenced outputs. This study provides a comprehensive comparison of machine-learning techniques for classification purposes in soil science and may assist in model selection for digital soil mapping and geomorphic modeling studies in the future.},
archivePrefix = {arXiv},
arxivId = {1605.08695},
author = {Heung, Brandon and Ho, Hung Chak and Zhang, Jin and Knudby, Anders and Bulmer, Chuck E. and Schmidt, Margaret G.},
doi = {10.1016/j.geoderma.2015.11.014},
eprint = {1605.08695},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Heung et al. - 2016 - An overview and comparison of machine-learning techniques for classification purposes in digital soil mapping.pdf:pdf},
isbn = {0016-7061},
issn = {00167061},
journal = {Geoderma},
keywords = {Data-mining,Digital soil mapping,Machine-learning,Model comparison,Soil classification},
pages = {62--77},
pmid = {1000253691},
title = {{An overview and comparison of machine-learning techniques for classification purposes in digital soil mapping}},
url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi},
volume = {265},
year = {2016}
}
@inproceedings{Singla2016,
abstract = {Recent past has seen a lot of developments in the field of image-based dietary assessment. Food image classification and recognition are crucial steps for dietary assessment. In the last couple of years, advancements in the deep learning and convolutional neural networks proved to be a boon for the image classification and recognition tasks, specifically for food recognition because of the wide variety of food items. In this paper, we report experiments on food/non-food clas-sification and food recognition using a GoogLeNet model based on deep convolutional neural network. The experi-ments were conducted on two image datasets created by our own, where the images were collected from existing image datasets, social media, and imaging devices such as smart phone and wearable cameras. Experimental results show a high accuracy of 99.2{\%} on the food/non-food classification and 83.6{\%} on the food category recognition.},
author = {Singla, Ashutosh and Yuan, Lin and Ebrahimi, Touradj},
booktitle = {Proceedings of the 2nd International Workshop on Multimedia Assisted Dietary Management - MADiMa '16},
doi = {10.1145/2986035.2986039},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Singla, Yuan, Ebrahimi - 2016 - FoodNon-food Image Classification and Food Categorization using Pre-Trained GoogLeNet Model.pdf:pdf},
isbn = {9781450345200},
keywords = {Caffe,GoogLeNet,convolutional neural network (CNN),deep learning,food recognition,food/non-food classification},
pages = {3--11},
title = {{Food/Non-food Image Classification and Food Categorization using Pre-Trained GoogLeNet Model}},
url = {https://infoscience.epfl.ch/record/221610/files/madima2016{\_}food{\_}recognition.pdf http://dl.acm.org/citation.cfm?doid=2986035.2986039},
year = {2016}
}
@inproceedings{Szegedy2015,
abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - Unknown - Going deeper with convolutions.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
pages = {1--9},
pmid = {24920543},
title = {{Going deeper with convolutions}},
url = {https://arxiv.org/pdf/1409.4842.pdf},
volume = {07-12-June},
year = {2015}
}
@article{Yang2010,
abstract = {Food recognition is difficult because food items are de-formable objects that exhibit significant variations in ap-pearance. We believe the key to recognizing food is to ex-ploit the spatial relationships between different ingredients (such as meat and bread in a sandwich). We propose a new representation for food items that calculates pairwise statistics between local features computed over a soft pixel-level segmentation of the image into eight ingredient types. We accumulate these statistics in a multi-dimensional his-togram, which is then used as a feature vector for a discrim-inative classifier. Our experiments show that the proposed representation is significantly more accurate at identifying food than existing methods.},
author = {Yang, Shulin and Chen, Mei and Pomerleau, Dean and Sukthankar, Rahul},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang et al. - Unknown - Food Recognition Using Statistics of Pairwise Local Features.pdf:pdf},
title = {{Food Recognition Using Statistics of Pairwise Local Features}},
url = {https://homes.cs.washington.edu/{~}shapiro/cvpr10.pdf},
year = {2010}
}
@article{Joutou2009,
abstract = {Since health care on foods is drawing people's attention recently, a system that can record everyday meals easily is being awaited. In this paper, we propose an automatic food image recognition system for recording people's eating habits. In the proposed system, we use the Multiple Kernel Learning (MKL) method to integrate several kinds of image features such as color, texture and SIFT adaptively. MKL enables to estimate optimal weights to combine image features for each category. In addition, we implemented a prototype system to recognize food images taken by cellular-phone cameras. In the experiment, we have achieved the 61.34{\%} classification rate for 50 kinds of foods. To the best of our knowledge, this is the first report of a food image classification system which can be applied for practical use.},
author = {Joutou, Taichi and Yanai, Keiji},
doi = {10.1109/ICIP.2009.5413400},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Joutou, Yanai - Unknown - A FOOD IMAGE RECOGNITION SYSTEM WITH MULTIPLE KERNEL LEARNING.pdf:pdf},
isbn = {978-1-4244-5655-0},
issn = {1522-4880},
journal = {Image Processing (ICIP), 2009 16th IEEE International Conference on},
keywords = {food image,generic object recognition,multiple kernel learning},
pages = {285--288},
title = {{A food image recognition system with Multiple Kernel Learning}},
url = {http://img.cs.uec.ac.jp/pub/conf09/091110joutou{\_}0.pdf http://ieeexplore.ieee.org/document/5413400/ http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5413400},
year = {2009}
}
@article{Divakaran2016,
abstract = {A food recognition assistant system includes technologies to recognize foods and combinations of foods depicted in a digital picture of food. Some embodiments include technologies to estimate portion size and calories, and to estimate nutritional value of the foods. In some embodiments, data identifying recognized foods and related information are generated in an automated fashion without relying on human assistance to identify the foods. In some embodiments, the system includes technologies for achieving automatic food detection and recognition in a real-life setting with a cluttered background, without the images being taken in a controlled lab setting, and without requiring additional user input (such as user-defined bounding boxes). Some embodiments of the system include technologies for personalizing the food classification based on user-specific habits, location and/or other criteria.},
author = {Divakaran, A and Zhang, W and Yu, Q and Sawhney, H},
journal = {U.S. Patent Application Publication},
title = {{Automated food recognition and nutritional estimation with a personal mobile electronic device; Pub No.: US 2016/0063734 A1}},
url = {https://patents.google.com/patent/US20160063692?oq=Automated+food+recognition+and+nutritional+estimation+with+a+personal+mobile+electronic+device.},
year = {2016}
}
@inproceedings{Hassannejad2016,
abstract = {We evaluated the effectiveness in classifying food images of a deep-learning approach based on the specifications of Google's image recognition architecture Inception. The ar-chitecture is a deep convolutional neural network (DCNN) having a depth of 54 layers. In this study, we fine-tuned this architecture for classifying food images from three well-known food image datasets: ETH Food-101, UEC FOOD 100, and UEC FOOD 256. On these datasets we achieved, respectively, 88.28{\%}, 81.45{\%}, and 76.17{\%} as top-1 accuracy and 96.88{\%}, 97.27{\%}, and 92.58{\%} as top-5 accuracy. To the best of our knowledge, these results significantly improve the best published results obtained on the same datasets, while requiring less computation power, since the number of parameters and the computational complexity are much smaller than the competitors'. Because of this, even if it is still rather large, the deep network based on this ar-chitecture appears to be at least closer to the requirements for mobile systems.},
address = {New York, New York, USA},
author = {Hassannejad, Hamid and Matrella, Guido and Ciampolini, Paolo and {De Munari}, Ilaria and Mordonini, Monica and Cagnoni, Stefano},
booktitle = {Proceedings of the 2nd International Workshop on Multimedia Assisted Dietary Management - MADiMa '16},
doi = {10.1145/2986035.2986042},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hassannejad et al. - 2016 - Food Image Recognition Using Very Deep Convolutional Networks.pdf:pdf},
isbn = {9781450345200},
keywords = {automatic diet monitoring,convolutional neural networks (CNN),food recognition,inception},
pages = {41--49},
publisher = {ACM Press},
title = {{Food Image Recognition Using Very Deep Convolutional Networks}},
url = {http://dl.acm.org/citation.cfm?doid=2986035.2986042},
year = {2016}
}
@inproceedings{Kawano2014,
abstract = {We propose a mobile food recognition system the purposes of which are estimating calorie and nutritious of foods and recording a user's eating habits. Since all the processes on image recognition performed on a smartphone, the system does not need to send images to a server and runs on an ordinary smartphone in a real-time way. To recognize food items, a user draws bounding boxes by touching the screen first, and then the system starts food item recognition within the indicated bounding boxes. To recognize them more accurately, we segment each food item region by GrubCut, extract a color histogram and SURF-based bag-of-features, and finally classify it into one of the fifty food categories with linear SVM and fast chi(2) kernel. In addition, the system estimates the direction of food regions where the higher SVM output score is expected to be obtained, show it as an arrow on the screen in order to ask a user to move a smartphone camera. This recognition process is performed repeatedly about once a second. We implemented this system as an Android smartphone application so as to use multiple CPU cores effectively for real-time recognition. In the experiments, we have achieved the 81.55{\%} classification rate for the top 5 category candidates when the ground-truth bounding boxes are given. In addition, we obtained positive evaluation by user study compared to the food recording system without object recognition.},
author = {Kawano, Yoshiyuki and Yanai, Keiji},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-04117-9_38},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kawano, Yanai - Unknown - FoodCam A Real-Time Mobile Food Recognition System Employing Fisher Vector.pdf:pdf},
isbn = {9783319041162},
issn = {03029743},
number = {PART 2},
pages = {369--373},
title = {{FoodCam: A real-time mobile food recognition system employing Fisher Vector}},
url = {https://pdfs.semanticscholar.org/dee5/ef5e6c23b5d6dfe1885a4593103b873eb1c5.pdf},
volume = {8326 LNCS},
year = {2014}
}
@article{Ciocca2017,
abstract = {We propose a new dataset for the evaluation of food recognition algorithms that can be used in dietary monitoring applications. Each image depicts a real canteen tray with dishes and foods arranged in different ways. Each tray contains multiple instances of food classes. The dataset contains 1,027 canteen trays for a total of 3,616 food instances belonging to 73 food classes. The food on the tray images have been manually segmented using carefully drawn polygonal boundaries. We have benchmarked the dataset by designing an automatic tray analysis pipeline that takes a tray image as input, finds the regions of interest, and predicts for each region the corresponding food class. We have experimented three different classification strategies using also several visual descriptors. We achieve about 79{\%} of food and tray recognition accuracy using Convolutional-Neural-Networksbased features. The dataset, as well as the benchmark framework, are available to the research community.},
author = {Ciocca, Gianluigi and Napoletano, Paolo and Schettini, Raimondo},
doi = {10.1109/JBHI.2016.2636441},
issn = {21682194},
journal = {IEEE Journal of Biomedical and Health Informatics},
keywords = {Algorithm benchmarking,convolutional neural networks (CNN),food dataset,food recognition},
month = {may},
number = {3},
pages = {588--598},
pmid = {27959824},
title = {{Food Recognition: A New Dataset, Experiments, and Results}},
url = {http://ieeexplore.ieee.org/document/7776769/},
volume = {21},
year = {2017}
}
@inproceedings{Bossard2014,
abstract = {Abstract In this paper we address the problem of automatically recognizing pictured dishes. To this end, we introduce a novel method to mine discriminative parts using Random Forests (rf), which allows us to mine for parts simultaneously for all classes and to share ... $\backslash$n},
author = {Bossard, Lukas and Guillaumin, Matthieu and {Van Gool}, Luc},
booktitle = {Eccv},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bossard, Guillaumin, Gool - Unknown - Food-101 – Mining Discriminative Components with Random Forests.pdf:pdf},
keywords = {Discriminative part mining,Food recognition,Image classification,Random Forest},
pages = {446--461},
title = {{Food-101--mining discriminative components with random forests}},
url = {http://www.vision.ee.ethz.ch/{~}lbossard/bossard{\_}eccv14{\_}food-101.pdf},
year = {2014}
}
@inproceedings{Matsuda2012,
abstract = {This paper presents an automatic multi-view food classification of a food intake assessment system on a smart phone. Food intake assessment plays important roles in obesity management, which has shown significant impacts on public healthcare. Conventional dietary record-based food intake assessment methods are not popularly applied due to their inconvenience and high reliance on human interactions. This paper presents a smart phone application, named DietCam, to recognize food intakes automatically. The major difficulties in food recognition from images come from uncertainties of food appearances and deformable nature of food especially when they are on a complex background environment. The proposed DietCam system utilizes a multi-view recognition method that separates every food by estimating the best perspective and recognizing them using a probabilistic method. The implemented DietCam system on an iPhone 4 platform showed improved performance compared with baseline methods for food recognition, with an average accuracy of 84{\%} for the selective regular shape foods.},
author = {Matsuda, Yuji and Hoashi, Hajime and Yanai, Keiji},
booktitle = {Proceedings - IEEE International Conference on Multimedia and Expo},
doi = {10.1109/ICME.2012.157},
isbn = {978-1-4673-1659-0},
issn = {19457871},
keywords = {multiple kernel learning,multiple-food image,region detection,window search},
month = {jul},
pages = {25--30},
publisher = {IEEE},
title = {{Recognition of multiple-food images by detecting candidate regions}},
url = {http://ieeexplore.ieee.org/document/6298369/},
year = {2012}
}
@article{Griffin2007,
abstract = {We introduce a challenging set of 256 object categories containing a total of 30607 images. The original Caltech-101 1 was collected by choosing a set of object categories, downloading examples from Google Images and then manually screening out all images that did not fit the category. Caltech-256 is collected in a similar manner with several improvements: a) the number of categories is more than doubled, b) the minimum number of images in any category is increased from 31 to 80, c) aftifacts due to image rotation are avoided and d) a new and larger clutter category is introduced for testing background rejection. We suggest several testing paradigms to measure classification performance, the benchmark the dataset using two simple metrics as well as a state-of-the-art spatial pyramid matching 2 algorithm. Finally we use the clutter category to train an interest detector which rejects uninformative background regions.},
author = {Griffin, Gregory and Holub, Alex and Perona, Pietro},
isbn = {UCB/CSD-04-1366},
journal = {Caltech mimeo},
keywords = {Caltech Library Services},
month = {mar},
number = {1},
pages = {20},
publisher = {California Institute of Technology},
title = {{Caltech-256 object category dataset}},
url = {https://authors.library.caltech.edu/7694/ http://authors.library.caltech.edu/7694},
volume = {11},
year = {2007}
}
@article{Fei-Fei2004,
abstract = {— Current computational approaches to learning vi-sual object categories require thousands of training images, are slow, cannot learn in an incremental manner and cannot incorporate prior information into the learning process. In addition, no algorithm presented in the literature has been tested on more than a handful of object categories. We present an method for learning object categories from just a few training images. It is quick and it uses prior information in a principled way. We test it on a dataset composed of images of objects belonging to 101 widely varied categories. Our proposed method is based on making use of prior information, assembled from (unrelated) object categories which were previously learnt. A generative probabilistic model is used, which represents the shape and appearance of a constellation of features belonging to the object. The parameters of the model are learnt incrementally in a Bayesian manner. Our incremental algorithm is compared experimentally to an earlier batch Bayesian algorithm, as well as to one based on maximum-likelihood. The incremental and batch versions have comparable classification performance on small training sets, but incremental learning is significantly faster, making real-time learning feasible. Both Bayesian methods outperform maximum likelihood on small training sets.},
author = {Fei-Fei, Li and Fergus, Rob and Perona, Pietro},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fei-Fei, Fergus, Perona - Unknown - Learning Generative Visual Models from Few Training Examples An Incremental Bayesian Approach Tested.pdf:pdf},
journal = {Proceedings of the 2004 Computer Vision and Pattern Recognition Workshop on Generative-Model Based Vision},
title = {{Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories}},
url = {http://people.csail.mit.edu/fergus/papers/Fei-Fei{\_}GMBV04.pdf},
year = {2004}
}
@article{Everingham2010,
abstract = {The PASCAL Visual Object Classes (VOC) chal-lenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and anno-tation, and standard evaluation procedures. Organised annu-ally from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection. This paper describes the dataset and evaluation proce-dure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the meth-ods are statistically different, what they are learning from the images (e.g. the object or its context), and what the meth-ods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension.},
author = {Everingham, Mark and {Van Gool}, Luc and {I Williams}, Christopher K and Winn, John and Zisserman, Andrew and Everingham, M and {Van Gool Leuven}, L KU and {CKI Williams}, Belgium and Winn, J and Zisserman, A},
doi = {10.1007/s11263-009-0275-4},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Everingham et al. - 2010 - The PASCAL Visual Object Classes (VOC) Challenge.pdf:pdf},
journal = {Int J Comput Vis},
keywords = {Benchmark {\textperiodcentered},Database {\textperiodcentered},Object detection,Object recognition {\textperiodcentered}},
pages = {303--338},
title = {{The PASCAL Visual Object Classes (VOC) Challenge}},
url = {http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham10.pdf},
volume = {88},
year = {2010}
}
@article{Caesar2016,
abstract = {Semantic classes can be either things (objects with a well-defined shape, e.g. car, person) or stuff (amorphous background regions, e.g. grass, sky). While lots of classification and detection works focus on thing classes, less attention has been given to stuff classes. Nonetheless, stuff classes are important as they allow to explain important aspects of an image, including (1) scene type; (2) which thing classes are likely to be present and their location (through contextual reasoning); (3) physical attributes, material types and geometric properties of the scene. To understand stuff and things in context we introduce COCO-Stuff, which augments all 164K images of the COCO 2017 dataset with pixel-wise annotations for 91 stuff classes. We introduce an efficient stuff annotation protocol based on superpixels, which leverages the original thing annotations. We quantify the speed versus quality trade-off of our protocol and explore the relation between annotation time and boundary complexity. Furthermore, we use COCO-Stuff to analyze: (a) the importance of stuff and thing classes in terms of their surface cover and how frequently they are mentioned in image captions; (b) the spatial relations between stuff and things, highlighting the rich contextual relations that make our dataset unique; (c) the performance of a modern semantic segmentation method on stuff and thing classes, and whether stuff is easier to segment than things.},
archivePrefix = {arXiv},
arxivId = {1612.03716},
author = {Caesar, Holger and Uijlings, Jasper and Ferrari, Vittorio},
eprint = {1612.03716},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Caesar, Uijlings, Ferrari - 2016 - COCO-Stuff Thing and Stuff Classes in Context.pdf:pdf},
issn = {1612.03716},
month = {dec},
title = {{COCO-Stuff: Thing and Stuff Classes in Context}},
url = {http://arxiv.org/abs/1612.03716},
year = {2016}
}
@inproceedings{Chen2009,
abstract = {We introduce the first visual dataset of fast foods with a total of 4,545 still images, 606 stereo pairs, 303 360° videos for structure from motion, and 27 privacy-preserving videos of eating events of volunteers. This work was motivated by research on fast food recognition for dietary assessment. The data was collected by obtaining three instances of 101 foods from 11 popular fast food chains, and capturing images and videos in both restaurant conditions and a controlled lab setting. We benchmark the dataset using two standard approaches, color histogram and bag of SIFT features in conjunction with a discriminative classifier. Our dataset and the benchmarks are designed to stimulate research in this area and will be released freely to the research community.},
author = {Chen, Mei and Dhingra, Kapil and Wu, Wen and Yang, Lei and Sukthankar, Rahul and Yang, Jie},
booktitle = {Proceedings - International Conference on Image Processing, ICIP},
doi = {10.1109/ICIP.2009.5413511},
isbn = {9781424456543},
issn = {15224880},
keywords = {Food image dataset,Object recognition},
month = {nov},
pages = {289--292},
publisher = {IEEE},
title = {{PFID: Pittsburgh Fast-food Image Dataset}},
url = {http://ieeexplore.ieee.org/document/5413511/},
year = {2009}
}
@article{Viola2001,
abstract = {This paper describes a visual object detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction of a new image representation called the “Integral Image” which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features and yields extremely efficient classifiers [6]. The third contribution is a method for combining classifiers in a “cascade” which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. A set of experiments in the domain of face detection are presented. The system yields face detection performace comparable to the best previous systems [18, 13, 16, 12, 1]. Implemented on a conventional desktop, face detection proceeds at 15 frames per second.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Viola, Paul and Jones, Michael},
doi = {http://dx.doi.org/10.1023/B:VISI.0000013087.49260.fb},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Viola, Jones - Unknown - Robust Real-time Object Detection.pdf:pdf},
isbn = {1094670599130},
issn = {09205691},
journal = {International Journal of Computer Vision},
number = {2},
pages = {137--154},
pmid = {7143246},
title = {{Robust real-time object detection}},
url = {https://www.cs.cmu.edu/{~}efros/courses/LBMV07/Papers/viola-IJCV-01.pdf http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Robust+Real-time+Object+Detection{\#}0},
volume = {57},
year = {2001}
}
@book{G.Berlage1988,
author = {{G. Berlage}, A and {M. Cooper}, T and {F. Aristazabal}, J},
booktitle = {Transactions of the ASAE},
doi = {10.13031/2013.30659},
month = {jan},
pages = {24--27},
title = {{Machine Vision Identification of Diploid and Tetraploid Ryegrass Seed}},
volume = {31},
year = {1988}
}
@article{Ding1994,
abstract = {Food material shape is often closely related to its quality. Due to the demands of high quality, automated food shape inspection has become an important need for the food industry. Currently, accuracy and speed are two major problems for food shape inspection with computer vision. Therefore, in this study, a fast and accurate computer-vision based feature extraction and classification system was developed. In the feature extraction stage, a statistical model-basedfeature extractor (SMB) and a multi-index active model-based (MAM) feature extractor were developed to improve the accuracy of classifications. In the classification stage, first the back-propagation neural network was applied as a multi-index classifier. Then, to speed up training, some minimum indeterminate zone (MIZ) classifiers were developed. Corn kernels, almonds, and animal-shaped crackers were used to test the above techniques. The results showed that accuracy and speed were greatly improved when the MAM feature extractor was used in conjunction with the MIZ classifier. Keywords. Corn, Crackers, Almonds, Image processing. Machine learning. Neural network. Feature extractors. Classifiers. M any foods, such as grains, fruits, and vegetables, have certain shape features that signify their overall quality. Thus damage to these foods usually causes some kind of profile or shape change. Therefore, shape inspection is widely used for food quality evaluation. Currently, shapes of many food products are inspected by human vision. It is slow and often difficult. Machine vision is a powerful tool for automated food shape inspection. It can provide objective, consistent, and quantitative information. However, accuracy and speed are still two major problems. Therefore, in this study, some new shape feature extraction and classification methods have been developed to improve the speed and accuracy of machine vision systems for automated food shape inspection. FEATURE EXTRACTION WITH COMPUTER VISION},
author = {Ding, K and Gunasekaran, S},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding, Gunasekaran - Unknown - SHAPE FEATURE EXTRACTION AND CLASSIFICATION OF FOOD MATERIAL USING COMPUTER VISION.pdf:pdf},
journal = {Society},
number = {5},
pages = {1537--1545},
title = {{SHAPE FEATURE EXTRACTION AND CLASSIFICATION OF FOOD MATERIAL USING COMPUTER VISION}},
url = {https://pdfs.semanticscholar.org/9082/bfae3280b40ff63f5f33537c1538f050b77c.pdf},
volume = {37},
year = {1994}
}
@article{NHS2015,
author = {NHS},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/NHS - 2015 - Patient Meal Ordering Procedure.pdf:pdf},
title = {{Patient Meal Ordering Procedure}},
url = {https://www.northdevonhealth.nhs.uk/wp-content/uploads/2015/08/Patient-Meal-Ordering-Procedure-V2.4-26.01.17.pdf},
year = {2015}
}
@article{Stratton2006,
abstract = {Malnutrition and its impact on clinical outcome may be underestimated in hospitalised elderly as many screening procedures require measurements of weight and height that cannot often be undertaken in sick elderly patients. The 'Malnutrition Universal Screening Tool' ('MUST') has been developed to screen all adults, even if weight and/or height cannot be measured, enabling more complete information on malnutrition prevalence and its impact on clinical outcome to be obtained. In the present study, 150 consecutively admitted elderly patients (age 85 (sd 5.5) years) were recruited prospectively, screened with 'MUST' and clinical outcome recorded. Although only 56 {\%} of patients could be weighed, all (n 150) could be screened with 'MUST'; 58 {\%} were at malnutrition risk and these individuals had greater mortality (in-hospital and post-discharge, P{\textless}0.01) and longer hospital stays (P=0.02) than those at low risk. Both 'MUST' categorisation and component scores (BMI, weight loss, acute disease) were significantly related to mortality (P{\textless}0.03). Those patients with no measured or recalled weight ('MUST' subjective criteria used) had a greater risk of malnutrition (P=0.01) and a poorer clinical outcome (P{\textless}0.002) than those who could be weighed and, within both groups, clinical outcome was worse in those at risk of malnutrition. The present study suggests that 'MUST' predicts clinical outcome in hospitalised elderly, in whom malnutrition is common (58 {\%}). In those who cannot be weighed, a higher prevalence of malnutrition and associated poorer clinical outcome supports the importance of routine screening with a tool, like 'MUST', that can be used to screen all patients.},
author = {Stratton, Rebecca J and King, Claire L and Stroud, Mike A and Jackson, Alan A and Elia, Marinos},
issn = {0007-1145},
journal = {The British journal of nutrition},
month = {feb},
number = {2},
pages = {325--30},
pmid = {16469149},
title = {{'Malnutrition Universal Screening Tool' predicts mortality and length of hospital stay in acutely ill elderly.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16469149},
volume = {95},
year = {2006}
}
@article{Dillon2012,
abstract = {In 2009, Morrison, the healthcare “food service” division of Compass Group Canada, introduced Steamplicity, an innovative meal and meal delivery system. Morrison claims enhanced patient satisfaction with meals, reporting scores of greater than 90{\%} satisfaction and tray waste reductions of 39{\%}. Patients are eating what they order. Reports of the waste typically generated by a hospital's food service operations are indicating Carbon dioxide equivalent (CO2e) footprint reductions in the range of 60{\%} versus the environmental impact of previous meal delivery systems.},
author = {Dillon, Helen Ann and McDonald, Sharon and Jonus, Ilona},
doi = {10.1016/J.HCMF.2012.07.010},
issn = {0840-4704},
journal = {Healthcare Management Forum},
month = {sep},
number = {3},
pages = {S20--S28},
publisher = {No longer published by Elsevier},
title = {{Steamplicity: An innovative meal system that delivers}},
url = {https://www.sciencedirect.com/science/article/pii/S0840470412000968},
volume = {25},
year = {2012}
}
@article{Edwards,
abstract = {Patient meals are an integral part of treatment hence the provision and consumption of a balanced diet, essential to aid recovery. A number of food service systems are used to provide meals but recently, the 'Steamplicity' concept has been introduced. This seeks, through the application of a static, extended choice menu, revised patient ordering procedures, new cooking processes and individual patient food cooked at ward level, to address some of the current hospital food service concerns. The purpose of this study was to directly compare selected aspects (food wastage at ward level; satisfaction with systems and food provided) of a traditional cook-chill food service operation against 'Steamplicity'. Results indicate that patients preferred the 'Steamplicty' system in all areas: food choice, ordering, delivery, food quality and overall. Wastage was considerably less with the 'Steamplicity' system; although care must be taken to ensure that poor operating procedures do not negate this advantage. When the total weight of food consumed in the ward at each meal is divided by the number of main courses served, results show that at lunch, mean intake with the cook-chill system was 202g whilst that for the 'Steamplicity' system was 282g and for the evening meal, 226g compared with 310g.},
author = {Edwards, J S A and Hartwell, H J},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Edwards, Hartwell - Unknown - Hospital foodservice a comparative analysis of systems and introducing the 'Steamplicity' concept.pdf:pdf},
keywords = {060,7,food intake Word count,institution,satisfaction,wastage},
title = {{Hospital foodservice: a comparative analysis of systems and introducing the 'Steamplicity' concept}},
url = {https://pdfs.semanticscholar.org/3535/8b7b8f1545123a390ef66bebe8640730fec9.pdf}
}
@article{,
abstract = {sima ajami, tayyebe Bagheri-tadi department of Health information technology, Health management {\&} economics research center, school of medical management and information sciences, isfahan university of medical sciences, isfahan, iran corresponding author: tayyebe Bagheri-tadi. school of medical management and information sciences, isfahan university of medical sciences, isfahan, iran. e-mail: bagheri774@yahoo.com.},
doi = {10.5455/aim.2013.21.129-134},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2013 - Barriers for Adopting Electronic Health Records (EHRs) by Physicians.pdf:pdf},
number = {2},
pages = {129--134},
title = {{Barriers for Adopting Electronic Health Records (EHRs) by Physicians}},
url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3766548/pdf/aim-21-2-13.pdf},
volume = {21},
year = {2013}
}
@article{Nilsen2016,
abstract = {Background: Industrialized and welfare societies are faced with vast challenges in the field of healthcare in the years to come. New technological opportunities and implementation of welfare technology through co-creation are considered part of the solution to this challenge. Resistance to new technology and resistance to change is, however, assumed to rise from employees, care receivers and next of kin. The purpose of this article is to identify and describe forms of resistance that emerged in five municipalities during a technology implementation project as part of the care for older people. Methods: This is a longitudinal, single-embedded case study with elements of action research, following an implementation of welfare technology in the municipal healthcare services. Participants included staff from the municipalities, a network of technology developers and a group of researchers. Data from interviews, focus groups and participatory observation were analysed. Results: Resistance to co-creation and implementation was found in all groups of stakeholders, mirroring the complexity of the municipal context. Four main forms of resistance were identified: 1) organizational resistance, 2) cultural resistance, 3) technological resistance and 4) ethical resistance, each including several subforms. The resistance emerges from a variety of perceived threats, partly parallel to, partly across the four main forms of resistance, such as a) threats to stability and predictability (fear of change), b) threats to role and group identity (fear of losing power or control) and c) threats to basic healthcare values (fear of losing moral or professional integrity). Conclusion: The study refines the categorization of resistance to the implementation of welfare technology in healthcare settings. It identifies resistance categories, how resistance changes over time and suggests that resistance may play a productive role when the implementation is organized as a co-creation process. This indicates that the importance of organizational translation between professional cultures should not be underestimated, and supports research indicating that focus on co-initiation in the initial phase of implementation projects may help prevent different forms of resistance in complex co-creation processes.},
author = {Nilsen, Etty R and Dugstad, Janne and Eide, Hilde and Gullslett, Monika Knudsen and Eide, Tom},
doi = {10.1186/s12913-016-1913-5},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nilsen et al. - 2016 - Exploring resistance to implementation of welfare technology in municipal healthcare services – a longitudinal ca.pdf:pdf},
journal = {BMC Health Services Research},
keywords = {Co-creation,Ethical resistance,Innovation,Municipal healthcare,Welfare technology},
title = {{Exploring resistance to implementation of welfare technology in municipal healthcare services – a longitudinal case study}},
url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5111186/pdf/12913{\_}2016{\_}Article{\_}1913.pdf},
volume = {16},
year = {2016}
}
@inproceedings{Galletta2017,
author = {Galletta, Antonino and Bonanno, Lilla and Celesti, Antonio and Marino, Silvia and Bramanti, Placido and Villari, Massimo},
booktitle = {2017 IEEE Symposium on Computers and Communications (ISCC)},
doi = {10.1109/ISCC.2017.8024511},
isbn = {978-1-5386-1629-1},
month = {jul},
pages = {94--99},
publisher = {IEEE},
title = {{An approach to share MRI data over the Cloud preserving patients' privacy}},
url = {http://ieeexplore.ieee.org/document/8024511/},
year = {2017}
}
@inproceedings{Celesti2013,
author = {Celesti, Antonio and Peditto, Nicola and Verboso, Fabio and Villari, Massimo and Puliafito, Antonio},
booktitle = {2013 IEEE International Symposium on Parallel {\&} Distributed Processing, Workshops and Phd Forum},
doi = {10.1109/IPDPSW.2013.266},
isbn = {978-0-7695-4979-8},
month = {may},
pages = {1490--1497},
publisher = {IEEE},
title = {{DRACO PaaS: A Distributed Resilient Adaptable Cloud Oriented Platform}},
url = {http://ieeexplore.ieee.org/document/6651043/},
year = {2013}
}
@misc{,
title = {{Maxwell MRI Case Study  |  Google Cloud Platform}},
url = {https://cloud.google.com/customers/maxwell-mri/},
urldate = {2018-03-08}
}
@book{Montgomery,
abstract = {Sixth edition. "This best-selling engineering statistics text provides a practical approach that is more oriented to engineering and the chemical and physical sciences than many similar texts. It is packed with unique problem sets that reflect realistic situations engineers will encounter in their working lives. This text shows how statistics, the science of data is just as important for engineers as the mechanical, electrical, and materials sciences"--},
author = {Montgomery, Douglas C. and Runger, George C.},
isbn = {9781118539712},
pages = {811},
title = {{Applied statistics and probability for engineers}},
url = {http://www.slader.com/textbook/9781118539712-applied-statistics-and-probability-for-engineers-6th-edition/}
}
@article{Sonnino2011,
abstract = {This article aims to address the need for more comprehensive studies on sustainable food systems through a case study of hospital food waste in Wales, UK. Based on a mixed-method research approach that focused on the links between hospital food waste, catering practices and public procurement strategies, the article shows that the hospital meal system, in the case studied, is responsible for overall levels of food waste that greatly exceed the official percentages provided by the Health Board. In addition to showing the theoretical benefits of research that accounts for the complex interrelations between different stages of the food chain, the study raises the need for a more integrated political approach that mobilizes all actors in the food system around a shared vision for sustainable development.},
author = {Sonnino, Roberta and McWilliam, Susannah},
doi = {10.1016/J.FOODPOL.2011.09.003},
issn = {0306-9192},
journal = {Food Policy},
month = {dec},
number = {6},
pages = {823--829},
publisher = {Pergamon},
title = {{Food waste, catering practices and public procurement: A case study of hospital food systems in Wales}},
url = {http://www.sciencedirect.com/science/article/pii/S0306919211001163},
volume = {36},
year = {2011}
}
@article{Xia2006,
abstract = {AIM This paper describes the findings of a descriptive study about what nurses do at mealtimes in relation to monitoring/assisting the eating practices of older patients in an acute care facility. BACKGROUND The prevalence of under nutrition is known to be high in hospitalized older patients and insufficient dietary intake is regarded as a major cause. However, most of the research tends to concentrate on the nursing home setting. Little is known about the situation in acute care facilities. METHODS Two medical wards participated in the study. Ward 1 had introduced a change of nurses' meal break time and ward 2 continued with normal practice. Convenience sampling was used. Fifty nurses and 48 patients were observed at different mealtimes during two weeks. Four nurses and four patients who were observed were also interviewed. Data were analysed using descriptive statistics and thematic analysis. RESULTS Kitchen staff delivered all meals and collected the majority of the meal trays. Older patients did not receive enough assistance during mealtimes. Interruptions happened frequently and social interaction was neglected. About one-third of patients observed left more than two-third of their meals. CONCLUSION Nutrition issues appeared to receive less priority in the ward than other nursing care activities and nurses' assistance was generally insufficient and not provided in a timely manner. Relevance to clinical practice. Findings highlight the deficiency in practice that should suggest to nurses that they examine their practice and put into place strategies to ensure older patients are properly/adequately hydrated and receive sufficient nutrient intake.},
author = {Xia, Chenfan and McCutcheon, Helen},
doi = {10.1111/j.1365-2702.2006.01425.x},
issn = {0962-1067},
journal = {Journal of Clinical Nursing},
month = {oct},
number = {10},
pages = {1221--1227},
pmid = {16968426},
title = {{Mealtimes in hospital ? who does what?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16968426 http://doi.wiley.com/10.1111/j.1365-2702.2006.01425.x},
volume = {15},
year = {2006}
}
@article{Budiningsari2016,
abstract = {BACKGROUND/OBJECTIVES Monitoring food intake of patients during hospitalization using simple methods and minimal training is an ongoing problem in hospitals. Therefore, there is a need to develop and validate a simple, easy to use, and quick tool that enables staff to estimate dietary intake. Thus, this study aimed to develop and validate the Pictorial Dietary Assessment Tool (PDAT). SUBJECTS AND METHODS A total of 37 health care staff members consisting of dietitians, nurses, and serving assistants estimated 130 breakfast and lunch meals consumed by 67 patients using PDAT. PDAT was developed based on the hospital menu that consists of staple food (rice or porridge), animal source protein (chicken, meat, eggs, and fish), and non-animal source protein (tau fu and tempeh), with a total of six pictorials of food at each meal time. Weighed food intake was used as a gold standard to validate PDAT. Agreement between methods was analyzed using correlations, paired t-test, Bland-Altman plots, kappa statistics, and McNemar's test. Sensitivity, specificity, and area under the curve of receiver operating characteristic were calculated to identify whether patients who had an inadequate food intake were categorized as at risk by the PDAT, based on the food weighing method. Agreement between different backgrounds of health care staff was calculated by intraclass correlation coefficient and analysis of variance test. RESULTS There was a significant correlation between the weighing food method and PDAT for energy (r=0.919, P{\textless}0.05), protein (r=0.843, P{\textless}0.05), carbohydrate (r=0.912, P{\textless}0.05), and fat (r=0.952; P{\textless}0.05). Nutrient intakes as assessed using PDAT and food weighing were rather similar (295±163 vs 292±158 kcal for energy; 13.9±7.8 vs 14.1±8.0 g for protein; 46.1±21.4 vs 46.7±22.3 g for carbohydrate; 7.4±3.1 vs 7.4±3.1 g for fat; P{\textgreater}0.05). The PDAT and food weighing method showed a satisfactory agreement beyond chance (k) (0.81 for staple food and animal source protein; 0.735 for non-animal source protein). Intraclass correlation coefficient ranged between 0.91 and 0.96 among respondents. There were no differences in energy, protein, carbohydrate, and fat intake estimated among health care staff (P=0.967; P=0.951; P=0.888; P=0.847, respectively). CONCLUSION In conclusion, PDAT provides a valid estimation of macronutrient consumption among hospitalized adult patients.},
author = {Budiningsari, Dwi and Shahar, Suzana and Manaf, Zahara Abdul and Susetyowati, Susetyowati},
doi = {10.2147/JMDH.S105000},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Budiningsari et al. - 2016 - A simple dietary assessment tool to monitor food intake of hospitalized adult patients.pdf:pdf},
issn = {1178-2390},
journal = {Journal of multidisciplinary healthcare},
keywords = {dietary assessment tool,hospitalized patients,nutrient intake,plate waste,validation},
pages = {311--22},
pmid = {27555779},
publisher = {Dove Press},
title = {{A simple dietary assessment tool to monitor food intake of hospitalized adult patients.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27555779 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4968985},
volume = {9},
year = {2016}
}
@article{Norman2008,
abstract = {This review focuses on the studies investigating the prognostic implications of disease-related malnutrition. Malnutrition is a common problem in patients with chronic or severe diseases. Prevalence of hospital malnutrition ranges between 20{\%} and 50{\%} depending on the criteria used in order to determine malnutrition and the patient's characteristics. Furthermore, nutritional status is known to worsen during hospital stay which is partly due to the poor recognition by the medical staff and adverse clinical routines. Studies have repeatedly shown that clinical malnutrition however has serious implications for recovery from disease, trauma and surgery and is generally associated with increased morbidity and mortality both in acute and chronic diseases. Length of hospital stay is significantly longer in malnourished patients and higher treatment costs are reported in malnutrition. Since it has been demonstrated that proper nutritional care can reduce the prevalence of hospital malnutrition and costs, nutritional assessment is mandatory in order to recognise malnutrition early and initiate timely nutritional therapy.},
author = {Norman, Kristina and Pichard, Claude and Lochs, Herbert and Pirlich, Matthias},
doi = {10.1016/j.clnu.2007.10.007},
issn = {02615614},
journal = {Clinical Nutrition},
month = {feb},
number = {1},
pages = {5--15},
pmid = {18061312},
title = {{Prognostic impact of disease-related malnutrition}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18061312 http://linkinghub.elsevier.com/retrieve/pii/S0261561407001689},
volume = {27},
year = {2008}
}
@article{Williams2011,
author = {Williams, Peter and Walton, Karen},
doi = {10.1016/j.eclnm.2011.09.006},
issn = {17514991},
journal = {e-SPEN, the European e-Journal of Clinical Nutrition and Metabolism},
month = {dec},
number = {6},
pages = {e235--e241},
title = {{Plate waste in hospitals and strategies for change}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1751499111000588},
volume = {6},
year = {2011}
}
@article{Hiesmayr2009,
abstract = {BACKGROUND {\&} AIMS Malnutrition is a known risk factor for the development of complications in hospitalised patients. We determined whether eating only fractions of the meals served is an independent risk factor for mortality. METHODS The NutritionDay is a multinational one-day cross-sectional survey of nutritional factors and food intake in 16,290 adult hospitalised patients on January 19th 2006. The effect of food intake and nutritional factors on death in hospital within 30 days was assessed in a competing risk analysis. RESULTS More than half of the patients did not eat their full meal provided by the hospital. Decreased food intake on NutritionDay or during the previous week was associated with an increased risk of dying, even after adjustment for various patient and disease related factors. Adjusted hazard ratio for dying when eating about a quarter of the meal on NutritionDay was 2.10 (1.53-2.89); when eating nothing 3.02 (2.11-4.32). More than half of the patients who ate less than a quarter of their meal did not receive artificial nutrition support. Only 25{\%} patients eating nothing at lunch receive artificial nutrition support. CONCLUSION Many hospitalised patients in European hospitals eat less food than provided as regular meal. This decreased food intake represents an independent risk factor for hospital mortality.},
author = {Hiesmayr, M. and Schindler, K. and Pernicka, E. and Schuh, C. and Schoeniger-Hekele, A. and Bauer, P. and Laviano, A. and Lovell, A.D. and Mouhieddine, M. and Schuetz, T. and Schneider, S.M. and Singer, P. and Pichard, C. and Howard, P. and Jonkers, C. and Grecu, I. and Ljungqvist, O. and {NutritionDay Audit Team}},
doi = {10.1016/j.clnu.2009.05.013},
issn = {02615614},
journal = {Clinical Nutrition},
month = {oct},
number = {5},
pages = {484--491},
pmid = {19573957},
title = {{Decreased food intake is a risk factor for mortality in hospitalised patients: The NutritionDay survey 2006}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19573957 http://linkinghub.elsevier.com/retrieve/pii/S0261561409001290},
volume = {28},
year = {2009}
}
@article{Agarwal2013,
abstract = {BACKGROUND {\&} AIMS The Australasian Nutrition Care Day Survey (ANCDS) ascertained if malnutrition and poor food intake are independent risk factors for health-related outcomes in Australian and New Zealand hospital patients. METHODS Phase 1 recorded nutritional status (Subjective Global Assessment) and 24-h food intake (0, 25, 50, 75, 100{\%} intake). Outcomes data (Phase 2) were collected 90-days post-Phase 1 and included length of hospital stay (LOS), readmissions and in-hospital mortality. RESULTS Of 3122 participants (47{\%} females, 65 ± 18 years) from 56 hospitals, 32{\%} were malnourished and 23{\%} consumed ≤ 25{\%} of the offered food. Malnourished patients had greater median LOS (15 days vs. 10 days, p {\textless} 0.0001) and readmissions rates (36{\%} vs. 30{\%}, p = 0.001). Median LOS for patients consuming ≤ 25{\%} of the food was higher than those consuming ≤ 50{\%} (13 vs. 11 days, p {\textless} 0.0001). The odds of 90-day in-hospital mortality were twice greater for malnourished patients (CI: 1.09-3.34, p = 0.023) and those consuming ≤ 25{\%} of the offered food (CI: 1.13-3.51, p = 0.017), respectively. CONCLUSION The ANCDS establishes that malnutrition and poor food intake are independently associated with in-hospital mortality in the Australian and New Zealand acute care setting.},
author = {Agarwal, Ekta and Ferguson, Maree and Banks, Merrilyn and Batterham, Marijka and Bauer, Judith and Capra, Sandra and Isenring, Elisabeth},
doi = {10.1016/j.clnu.2012.11.021},
issn = {02615614},
journal = {Clinical Nutrition},
keywords = {ANCDS,ARDRG,Australasian Nutrition Care Day Survey,Australian Refined Diagnosis Related Group,BMI,Body Mass Index,CI,Confidence Interval,DRG,Diagnosis Related Group,Disease type and severity,EQ-5D visual analogue scale,EQ-5Dvas,In-hospital mortality,LOS,Length of stay,MAJOR Diagnostic Category,MDC,MST,Malnutrition,Malnutrition Screening Tool,PCCL,Patient Clinical Complexity Level,Poor food intake,Readmissions,SGA,Subjective Global Assessment,length of stay},
month = {oct},
number = {5},
pages = {737--745},
pmid = {23260602},
title = {{Malnutrition and poor food intake are associated with prolonged hospital stay, frequent readmissions, and greater in-hospital mortality: Results from the Nutrition Care Day Survey 2010}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23260602 http://linkinghub.elsevier.com/retrieve/pii/S0261561412002695},
volume = {32},
year = {2013}
}
@article{Kwok2015,
author = {Kwok, Tsz-Ho and Tang, Kai},
doi = {10.1115/1.4031335},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kwok, Tang - 2015 - Improvements to the Iterative Closest Point Algorithm for Shape Registration in Manufacturing.pdf:pdf},
issn = {1087-1357},
journal = {Journal of Manufacturing Science and Engineering},
keywords = {Algorithms,Eigenvalues,Manufacturing,Rotation,Shapes,Stability,Torque},
month = {sep},
number = {1},
pages = {011014},
publisher = {American Society of Mechanical Engineers},
title = {{Improvements to the Iterative Closest Point Algorithm for Shape Registration in Manufacturing}},
url = {http://manufacturingscience.asmedigitalcollection.asme.org/article.aspx?doi=10.1115/1.4031335},
volume = {138},
year = {2015}
}
@article{Lauesen2001,
author = {Lauesen, S. and Harning, M.B.},
doi = {10.1109/MS.2001.936220},
issn = {0740-7459},
journal = {IEEE Software},
month = {jul},
number = {4},
pages = {67--75},
title = {{Virtual windows: linking user tasks, data models, and interface design}},
url = {http://ieeexplore.ieee.org/document/936220/},
volume = {18},
year = {2001}
}
@article{WorldHealthOrganisation2016,
author = {{World Health Organisation}},
journal = {WHO},
keywords = {FAQ [doctype],ageing [subject],cancer [subject],cervical cancer,clean water,elderly,geriatric,greywater,older people,safe water,wastewater,water [subject]},
publisher = {World Health Organization},
title = {{WHO | UV radiation}},
url = {http://www.who.int/uv/faq/whatisuv/en/},
year = {2016}
}
@article{Besl1992,
author = {Besl, P.J. and McKay, Neil D.},
doi = {10.1109/34.121791},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = {feb},
number = {2},
pages = {239--256},
title = {{A method for registration of 3-D shapes}},
url = {http://ieeexplore.ieee.org/document/121791/},
volume = {14},
year = {1992}
}
@misc{UKHealthandSafetyExecutive,
author = {{UK Health and Safety Executive}},
title = {{Skin at work: Glove sizes}},
url = {http://www.hse.gov.uk/skin/employ/glovesizes.htm},
urldate = {2017-08-02}
}
@misc{AMMEX,
author = {AMMEX},
title = {{Glove Size Chart | AMMEX Glove Size Chart}},
url = {https://ammex.com/ammex-glove-size-chart},
urldate = {2017-07-10}
}
@article{GlobusLtd.,
author = {{Globus Ltd.}},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Globus Ltd. - Unknown - Globus - Glove Sizing Chart.pdf:pdf},
title = {{Globus - Glove Sizing Chart}},
url = {http://www.showagloves.com/assets/files/Globus Hand Size Chart.pdf}
}
@article{SuperiorGlove.com,
author = {SuperiorGlove.com},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/SuperiorGlove.com - Unknown - Superior Glove - Sizing Chart.pdf:pdf},
title = {{Superior Glove - Sizing Chart}},
url = {http://www.superiorglove.com/pages/wp-content/uploads/Superior-Glove-Sizing-Guide1.pdf}
}
@misc{IntelDeveloperDocumentation,
author = {{Intel Developer Documentation}},
title = {{Intel{\textregistered} RealSense™ SDK 2016 R3 Documentation}},
url = {https://software.intel.com/sites/landingpage/realsense/camera-sdk/v2016r3/documentation/html/index.html?doc{\_}devguide{\_}introduction.html},
urldate = {2017-07-05}
}
@misc{IntelNewsroom,
author = {{Intel Newsroom}},
title = {{Intel Brings Immersive, Human Interaction to Devices in 2014 | Intel Newsroom}},
url = {https://newsroom.intel.com/news-releases/intel-brings-immersive-human-interaction-to-devices-in-2014/},
urldate = {2017-07-05}
}
@misc{GmbH,
author = {GmbH, IDS Imaging Development Systems},
title = {{USB 2 uEye SE industrial camera with CMOS sensors}},
url = {https://en.ids-imaging.com/store/products/cameras/usb-2-0-cameras/ueye-se/show/all.html{\#}},
urldate = {2017-07-30}
}
@misc{SchoolofComputerScience2016,
author = {{School of Computer Science}, Trinity College Dublin},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/School of Computer Science - 2016 - Master in Computer Engineering (MAI) Internship Programme Handbook.pdf:pdf},
title = {{Master in Computer Engineering (MAI) Internship Programme Handbook}},
url = {https://www.scss.tcd.ie/internships/handbook.pdf},
year = {2016}
}
@article{Zivkovic2004,
abstract = {Background subtraction is a common computer vision task. We analyze the usual pixel-level approach. We de-velop an efficient adaptive algorithm using Gaussian mix-ture probability density. Recursive equations are used to constantly update the parameters and but also to simulta-neously select the appropriate number of components for each pixel.},
author = {Zivkovic, Zoran},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zivkovic - 2004 - Improved Adaptive Gaussian Mixture Model for Background Subtraction.pdf:pdf},
title = {{Improved Adaptive Gaussian Mixture Model for Background Subtraction}},
url = {http://www.zoranz.net/Publications/zivkovic2004ICPR.pdf},
year = {2004}
}
@article{Stauffer,
abstract = {A common method for real-time segmentation of moving regions in image sequences involves " back-ground subtraction, " or thresholding the error between an estimate of the image without moving objects and the current image. The numerous approaches to this problem differ in the type of background model used and the procedure used to update the model. This paper discusses modeling each pixel as a mixture of Gaus-sians and using an on-line approximation to update the model. The Gaussian distributions of the adaptive mixture model are then evaluated to determine which are most likely to result from a background process. Each pixel is classified based on whether the Gaussian distribution which represents it most effectively is con-sidered part of the background model. This results in a stable, real-time outdoor tracker which reliably deals with lighting changes, repetitive motions from clutter, and long-term scene changes. This system has been run almost continuously for 16 months, 24 hours a day, through rain and snow.},
author = {Stauffer, Chris and Grimson, W E L},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stauffer, Grimson - Unknown - Adaptive background mixture models for real-time tracking.pdf:pdf},
title = {{Adaptive background mixture models for real-time tracking}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.88.877{\&}rep=rep1{\&}type=pdf}
}
@book{Dawson-Howe,
abstract = {"Explains the theory behind basic computer vision and provides a bridge from the theory to practical implementation using the industry standard OpenCV librariesComputer Vision is a rapidly expanding area and it is becoming progressively easier for developers to make use of this field due to the ready availability of high quality libraries (such as OpenCV 2). This text is intended to facilitate the practical use of computer vision with the goal being to bridge the gap between the theory and the practical implementation of computer vision. The book will explain how to use the relevant OpenCV library routines and will be accompanied by a full working program including the code snippets from the text. This textbook is a heavily illustrated, practical introduction to an exciting field, the applications of which are becoming almost ubiquitous. We are now surrounded by cameras, for example cameras on computers {\&} tablets/ cameras built into our mobile phones/ cameras in games consoles; cameras imaging difficult modalities (such as ultrasound, X-ray, MRI) in hospitals, and surveillance cameras. This book is concerned with helping the next generation of computer developers to make use of all these images in order to develop systems which are more intuitive and interact with us in more intelligent ways. Explains the theory behind basic computer vision and provides a bridge from the theory to practical implementation using the industry standard OpenCV libraries Offers an introduction to computer vision, with enough theory to make clear how the various algorithms work but with an emphasis on practical programming issues Provides enough material for a one semester course in computer vision at senior undergraduate and Masters levels Includes the basics of cameras and images and image processing to remove noise, before moving on to topics such as image histogramming; binary imaging; video processing to detect and model moving objects; geometric operations {\&} camera models; edge detection; features detection; recognition in images Contains a large number of vision application problems to provide students with the opportunity to solve real problems. Images or videos for these problems are provided in the resources associated with this book which include an enhanced eBook "-- "Explains the theory behind basic computer vision and provides a bridge from the theory to practical implementation using the industry standard OpenCV libraries"-- Machine generated contents note: 1. Introduction -- 1.1. A Difficult Problem -- 1.2. The Human Vision System -- 1.3. Practical Applications of Computer Vision -- 1.4. The Future of Computer Vision -- 1.5. Material in This Textbook -- 1.6. Going Further with Computer Vision -- 2. Images -- 2.1. Cameras -- 2.1.1. The Simple Pinhole Camera Model -- 2.2. Images -- 2.2.1. Sampling -- 2.2.2. Quantisation -- 2.3. Colour Images -- 2.3.1. Red-Green -- Blue (RGB) Images -- 2.5.2. Cyan-Magenta -- Yellow (CMY) Images -- 2.5.3. YUV Images -- 2.5.4. Hue Luminance Saturation (HLS) Images -- 2.5.5. Other Colour Spaces -- 2.5.6. Some Colour Applications -- 2.4. Noise -- 2.4.1. Types of Noise -- 2.4.2. Noise Models -- 2.4.3. Noise Generation -- 2.4.4. Noise Evaluation -- 2.5. Smoothing -- 2.5.1. Image Averaging -- 2.5.2. Local Averaging and Gaussian Smoothing -- 2.5.3. Rotating Mask -- 2.5.4. Median Filter -- 3. Histograms -- 3.1. 1D Histograms -- 3.1.1. Histogram Smoothing -- 3.1.2. Colour Histograms -- 3.2. 3D Histograms -- 3.3. Histogram/Image Equalisation -- 3.4. Histogram Comparison -- 3.5. Back-projection -- 3.6. k-means Clustering -- 4. Binary Vision -- 4.1. Thresholding -- 4.1.1. Thresholding Problems -- 4.2. Threshold Detection Methods -- 4.2.1. Bimodal Histogram Analysis -- 4.2.2. Optimal Thresholding -- 4.2.3. Otsu Thresholding -- 4.3. Variations on Thresholding -- 4.3.1. Adaptive Thresholding -- 4.3.2. Band Thresholding -- 4.3.3. Semi-thresholding -- 4.3.4. Multispectral Thresholding -- 4.4. Mathematical Morphology -- 4.4.1. Dilation -- 4.4.2. Erosion -- 4.4.3. Opening and Closing -- 4.4.4. Grey-scale and Colour Morphology -- 4.5. Connectivity -- 4.5.1. Connectedness: Paradoxes and Solutions -- 4.5.2. Connected Components Analysis -- 5. Geometric Transformations -- 5.1. Problem Specification and Algorithm -- 5.2. Affine Transformations -- 5.2.1. Known Affine Transformations -- 5.2.2. Unknown Affine Transformations -- 5.3. Perspective Transformations -- 5.4. Specification of More Complex Transformations -- 5.5. Interpolation -- 5.5.1. Nearest Neighbour Interpolation -- 5.5.2. Bilinear Interpolation -- 5.5.3. Bi-Cubic Interpolation -- 5.6. Modelling and Removing Distortion from Cameras -- 5.6.7. Camera Distortions -- 5.6.2. Camera Calibration and Removing Distortion -- 6. Edges -- 6.1. Edge Detection -- 6.1.1. First Derivative Edge Detectors -- 6.1.2. Second Derivative Edge Detectors -- 6.1.3. Multispectral Edge Detection -- 6.1.4. Image Sharpening -- 6.2. Contour Segmentation -- 6.2.1. Basic Representations of Edge Data -- 6.2.2. Border Detection -- 6.2.3. Extracting Line Segment Representations of Edge Contours -- 6.3. Hough Transform -- 6.3.1. Hough for Lines -- 6.3.2. Hough for Circles -- 6.3.3. Generalised Hough -- 7. Features -- 7.1. Moravec Corner Detection -- 7.2. Harris Corner Detection -- 7.3. FAST Corner Detection -- 7.4. SIFT -- 7.4.1. Scale Space Extrema Detection -- 7.4.2. Accurate Keypoint Location -- 7.4.3. Keypoint Orientation Assignment -- 7.4.4. Keypoint Descriptor -- 7.4.5. Matching Keypoints -- 7.4.6. Recognition -- 7.5. Other Detectors -- 7.5.1. Minimum Eigenvalues -- 7.5.2. SURF -- 8. Recognition -- 8.1. Template Matching -- 8.1.1. Applications -- 8.1.2. Template Matching Algorithm -- 8.1.3. Matching Metrics -- 8.1.4. Finding Local Maxima or Minima -- 8.1.5. Control Strategies for Matching -- 8.2. Chamfer Matching -- 8.2.1. Chamfering Algorithm -- 8.2.2. Chamfer Matching Algorithm -- 8.3. Statistical Pattern Recognition -- 8.3.1. Probability Review -- 8.3.2. Sample Features -- 8.3.3. Statistical Pattern Recognition Technique -- 8.4. Cascade of Haar Classifiers -- 8.4.1. Features -- 8.4.2. Training -- 8.4.3. Classifiers -- 8.4.4. Recognition -- 8.5. Other Recognition Techniques -- 8.5.1. Support Vector Machines (SVM) -- 8.5.2. Histogram of Oriented Gradients (HoG) -- 8.6. Performance -- 8.6.1. Image and Video Datasets -- 8.6.2. Ground Truth -- 8.6.3. Metrics for Assessing Classification Performance -- 8.6.4. Improving Computation Time -- 9. Video -- 9.1. Moving Object Detection -- 9.1.1. Object of Interest -- 9.1.2. Common Problems -- 9.1.3. Difference Images -- 9.1.4. Background Models -- 9.1.5. Shadow Detection -- 9.2. Tracking -- 9.2.1. Exhaustive Search -- 9.2.2. Mean Shift -- 9.2.3. Dense Optical Flow -- 9.2.4. Feature Based Optical Flow -- 9.3. Performance -- 9.3.1. Video Datasets (and Formats) -- 9.3.2. Metrics for Assessing Video Tracking Performance -- 10. Vision Problems -- 10.1. Baby Food -- 10.2. Labels on Glue -- 10.3. O-rings -- 10.4. Staying in Lane -- 10.5. Reading Notices -- 10.6. Mailboxes -- 10.7. Abandoned and Removed Object Detection -- 10.8. Surveillance -- 10.9. Traffic Lights -- 10.10. Real Time Face Tracking -- 10.11. Playing Pool -- 10.12. Open Windows -- 10.13. Modelling Doors -- 10.14. Determining the Time from Analogue Clocks -- 10.15. Which Page -- 10.16. Nut/Bolt/Washer Classification -- 10.17. Road Sign Recognition -- 10.18. License Plates -- 10.19. Counting Bicycles -- 10.20. Recognise Paintings.},
author = {Dawson-Howe, Kenneth.},
isbn = {1118848454},
title = {{A practical introduction to computer vision with OpenCV}}
}
@misc{SoftKinetics2012,
abstract = {An overview of the features of SoftKinetics DS325 camera.},
author = {SoftKinetics},
file = {:C$\backslash$:/Users/Paul/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/SoftKinetics - 2012 - DS325 Datasheet.pdf:pdf},
title = {{DS325 Datasheet}},
url = {http://www.softkinetic.com/Portals/0/Download/WEB{\_}20120907{\_}SK{\_}DS325{\_}Datasheet{\_}V2.1.pdf},
year = {2012}
}
@incollection{Baca2012,
author = {Baca, Miroslav and Grd, Petra and Fotak, Tomislav},
booktitle = {New Trends and Developments in Biometrics},
doi = {10.5772/51912},
month = {nov},
publisher = {InTech},
title = {{Basic Principles and Trends in Hand Geometry and Hand Shape Biometrics}},
url = {http://www.intechopen.com/books/new-trends-and-developments-in-biometrics/basic-principles-and-trends-in-hand-geometry-and-hand-shape-biometrics},
year = {2012}
}
@misc{,
title = {{About - OpenCV library}},
url = {http://opencv.org/about.html},
urldate = {2017-07-27}
}
@article{Klevens2007,
abstract = {OBJECTIVE The purpose of this study was to provide a national estimate of the number of healthcare-associated infections (HAI) and deaths in United States hospitals. METHODS No single source of nationally representative data on HAIs is currently available. The authors used a multi-step approach and three data sources. The main source of data was the National Nosocomial Infections Surveillance (NNIS) system, data from 1990-2002, conducted by the Centers for Disease Control and Prevention. Data from the National Hospital Discharge Survey (for 2002) and the American Hospital Association Survey (for 2000) were used to supplement NNIS data. The percentage of patients with an HAI whose death was determined to be caused or associated with the HAI from NNIS data was used to estimate the number of deaths. RESULTS In 2002, the estimated number of HAIs in U.S. hospitals, adjusted to include federal facilities, was approximately 1.7 million: 33,269 HAIs among newborns in high-risk nurseries, 19,059 among newborns in well-baby nurseries, 417,946 among adults and children in ICUs, and 1,266,851 among adults and children outside of ICUs. The estimated deaths associated with HAIs in U.S. hospitals were 98,987: of these, 35,967 were for pneumonia, 30,665 for bloodstream infections, 13,088 for urinary tract infections, 8,205 for surgical site infections, and 11,062 for infections of other sites. CONCLUSION HAIs in hospitals are a significant cause of morbidity and mortality in the United States. The method described for estimating the number of HAIs makes the best use of existing data at the national level.},
author = {Klevens, R. Monina and Edwards, Jonathan R. and Richards, Chesley L. and Horan, Teresa C. and Gaynes, Robert P. and Pollock, Daniel A. and Cardo, Denise M.},
doi = {10.1177/003335490712200205},
issn = {0033-3549},
journal = {Public Health Reports},
month = {mar},
number = {2},
pages = {160--166},
pmid = {17357358},
title = {{Estimating Health Care-Associated Infections and Deaths in U.S. Hospitals, 2002}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17357358 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC1820440 http://journals.sagepub.com/doi/10.1177/003335490712200205},
volume = {122},
year = {2007}
}
@article{Rosenthal2005,
abstract = {BACKGROUND Hand hygiene is a fundamental measure for the control of nosocomial infection. However, sustained compliance with hand hygiene in health care workers is poor. We attempted to enhance compliance with hand hygiene by implementing education, training, and performance feedback. We measured nosocomial infections in parallel. METHODS We monitored the overall compliance with hand hygiene during routine patient care in intensive care units (ICUs); 1 medical surgical ICU and 1 coronary ICU, of 1 hospital in Buenos Aires, Argentina, before and during implementation of a hand hygiene education, training, and performance feedback program. Observational surveys were done twice a week from September 2000 to May 2002. Nosocomial infections in the ICUs were identified using the National Nosocomial Infections Surveillance (NNIS) criteria, with prospective surveillance. RESULTS We observed 4347 opportunities for hand hygiene in both ICUs. Compliance improved progressively (handwashing adherence, 23.1{\%} (268/1160) to 64.5{\%} (2056/3187) (RR, 2.79; 95{\%} CI: 2.46-3.17; P {\textless} .0001). During the same period, overall nosocomial infection in both ICUs decreased from 47.55 per 1000 patient-days (104/2187) to 27.93 per 1000 patient days (207/7409) RR, 0.59; 95{\%} CI: 0.46-0.74, P {\textless} .0001). CONCLUSION A program consisting of focused education and frequent performance feedback produced a sustained improvement in compliance with hand hygiene, coinciding with a reduction in nosocomial infection rates in the ICUs.},
author = {Rosenthal, Victor D. and Guzman, Sandra and Safdar, Nasia},
doi = {10.1016/j.ajic.2004.08.009},
issn = {01966553},
journal = {American Journal of Infection Control},
month = {sep},
number = {7},
pages = {392--397},
pmid = {16153485},
title = {{Reduction in nosocomial infection with improved hand hygiene in intensive care units of a tertiary care hospital in Argentina}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16153485 http://linkinghub.elsevier.com/retrieve/pii/S019665530500430X},
volume = {33},
year = {2005}
}
@article{Boyce2002,
abstract = {The Guideline for Hand Hygiene in Health-Care Settings provides health-care workers (HCWs) with a review of data regarding handwashing and hand antisepsis in health-care settings. In addition, it provides specific recommendations to promote improved hand-hygiene practices and reduce transmission ofpathogenic microorganisms to patients and personnel in health-care settings. This report reviews studies published since the 1985 CDC guideline (Garner JS, Favero MS. CDC guideline for handwashing and hospital environmental control, 1985. Infect Control 1986;7:231-43) and the 1995 APIC guideline (Larson EL, APIC Guidelines Committee. APIC guideline for handwashing and hand antisepsis in health care settings. Am J Infect Control 1995;23:251-69) were issued and provides an in-depth review of hand-hygiene practices of HCWs, levels of adherence of personnel to recommended handwashing practices, and factors adversely affecting adherence. New studies of the in vivo efficacy of alcohol-based hand rubs and the low incidence of dermatitis associated with their use are reviewed. Recent studies demonstrating the value of multidisciplinary hand-hygiene promotion programs and the potential role of alcohol-based hand rubs in improving hand-hygiene practices are summarized. Recommendations concerning related issues (e.g., the use of surgical hand antiseptics, hand lotions or creams, and wearing of artificial fingernails) are also included.},
author = {Boyce, John M and Pittet, Didier and {Healthcare Infection Control Practices Advisory Committee} and {HICPAC/SHEA/APIC/IDSA Hand Hygiene Task Force}},
issn = {1057-5987},
journal = {MMWR. Recommendations and reports : Morbidity and mortality weekly report. Recommendations and reports},
month = {oct},
number = {RR-16},
pages = {1--45, quiz CE1--4},
pmid = {12418624},
title = {{Guideline for Hand Hygiene in Health-Care Settings. Recommendations of the Healthcare Infection Control Practices Advisory Committee and the HICPAC/SHEA/APIC/IDSA Hand Hygiene Task Force. Society for Healthcare Epidemiology of America/Association for Prof}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12418624},
volume = {51},
year = {2002}
}
@article{Berhe2006,
abstract = {BACKGROUND Infection control process measures provide actionable and measurable indicators for performance improvement. OBJECTIVE To determine the relationship between the measurement and feedback of selected infection control process measures and compliance with infection control practices. METHODS We measured selected infection control process measures (hand hygiene, femoral catheter use as a proportion of all central venous catheter (CVC) days and proportion of head of bed elevations) in the medical respiratory intensive care unit (ICU) (MRICU) and the surgical trauma ICU (STICU). All data were collected by trained infection control practitioners. Baseline data were obtained April through June 2004. Baseline hand hygiene data were obtained from May to June. Follow-up observations were obtained from July 2004 through March 2005. Both baseline and follow-up observations were reported to the units' leadership. The data were reviewed for improvement in compliance with process measures. Differences in proportions were analyzed for statistical significance by the chi(2) test. RESULTS There was a statistically significant improvement in the head of bed elevation rates: 54.9{\%} versus 98.4{\%} (P {\textless} .001) for the MRICU and 46.5{\%} versus 77.2{\%} (P {\textless} .001) for the STICU, respectively. There was also a statistically significant decline in femoral catheter rates in both ICUs: 17.8{\%} versus 10{\%} (P = .001) in the MRICU and 8.4{\%} versus 3{\%} (P {\textless} .001) in the STICU, respectively. There was no significant improvement in hand hygiene rates in either ICU: 31.8{\%} versus 39.3{\%} (P = .1) in the MRICU and 50{\%} versus 50.3{\%} (P = .9) in the STICU, respectively. CONCLUSION Feedback of process measures lowered the use of femoral catheters and improved the proportion of elevated head of beds in 2 ICUs, but there was no significant improvement in hand hygiene.},
author = {Berhe, Mezgebe and Edmond, Michael B. and Bearman, Gonzalo},
doi = {10.1016/j.ajic.2005.06.014},
issn = {01966553},
journal = {American Journal of Infection Control},
month = {oct},
number = {8},
pages = {537--539},
pmid = {17015162},
title = {{Measurement and feedback of infection control process measures in the intensive care unit: Impact on compliance}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17015162 http://linkinghub.elsevier.com/retrieve/pii/S0196655305008503},
volume = {34},
year = {2006}
}
@article{Higgins2013,
abstract = {BACKGROUND: In 2009, the World Health Organization recommended the use of a 'multi-faceted, multi-modal hand hygiene strategy' (Five Moments for Hand Hygiene) to improve hand hygiene compliance among healthcare workers. As part of this initiative, a training programme was implemented using an automated gaming technology training and audit tool to educate staff on hand hygiene technique in an acute healthcare setting. AIM: To determine whether using this automated training programme and audit tool as part of a multi-modal strategy would improve hand hygiene compliance and technique in an acute healthcare setting. METHODS: A time-series quasi-experimental design was chosen to measure compliance with the Five Moments for Hand Hygiene and handwashing technique. The study was performed from November 2009 to April 2012. An adenosine triphosphate monitoring system was used to measure handwashing technique, and SureWash (Glanta Ltd, Dublin, Ireland), an automated auditing and training unit, was used to provide assistance with staff training and education. FINDINGS: Hand hygiene technique and compliance improved significantly over the study period (P {\textless} 0.0001). CONCLUSION: Incorporation of new automated teaching technology into a hand hygiene programme can encourage staff participation in learning, and ultimately improve hand hygiene compliance and technique in the acute healthcare setting.},
author = {Higgins, A and Hannan, M M},
doi = {10.1016/j.jhin.2013.02.004},
issn = {1532-2939 (Electronic)},
journal = {The Journal of hospital infection},
keywords = {Adenosine Triphosphate,Cross Infection,Guideline Adherence,Hand Disinfection,Health Personnel,Humans,Hygiene,Infection Control,Ireland,World Health Organization,analysis,methods,standards,statistics {\&} numerical data},
language = {eng},
month = {may},
number = {1},
pages = {32--37},
pmid = {23498360},
title = {{Improved hand hygiene technique and compliance in healthcare workers using gaming technology.}},
volume = {84},
year = {2013}
}
@book{Larman2004,
abstract = {Iterative {\&} evolutionary -- Agile -- Story -- Motivation -- Evidence -- Scrum -- Extreme programming -- Unified process -- Evo -- Practice tips.},
author = {Larman, Craig.},
isbn = {0131111558},
pages = {342},
publisher = {Addison-Wesley},
title = {{Agile and iterative development : a manager's guide}},
year = {2004}
}
